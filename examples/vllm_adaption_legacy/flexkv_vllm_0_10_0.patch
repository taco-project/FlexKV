diff --git a/benchmarks/backend_request_func.py b/benchmarks/backend_request_func.py
index c7229dbb8..d2325fd3a 100644
--- a/benchmarks/backend_request_func.py
+++ b/benchmarks/backend_request_func.py
@@ -9,6 +9,7 @@ import time
 import traceback
 from dataclasses import dataclass, field
 from typing import Optional, Union
+import asyncio

 import aiohttp
 import huggingface_hub.constants
@@ -23,10 +24,10 @@ AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)

 @dataclass
 class RequestFuncInput:
-    prompt: str
+    prompt: Union[str, list[str]]
     api_url: str
-    prompt_len: int
-    output_len: int
+    prompt_len: Union[int, list[int]]
+    output_len: Union[int, list[int]]
     model: str
     model_name: Optional[str] = None
     logprobs: Optional[int] = None
@@ -555,6 +556,107 @@ async def async_request_openai_audio(
             pbar.update(1)
         return output

+async def async_request_openai_chat_completions_multiturns(
+    request_func_input: RequestFuncInput,
+    pbar: Optional[tqdm] = None,
+    turn_interval_time: float = 3.0,
+) -> RequestFuncOutput:
+    api_url = request_func_input.api_url
+    assert api_url.endswith(
+        ("chat/completions", "profile")
+    ), "OpenAI Chat Completions API URL must end with 'chat/completions'."
+    assert isinstance(request_func_input.prompt, list)
+    assert isinstance(request_func_input.prompt_len, list)
+    assert isinstance(request_func_input.output_len, list)
+
+    async with aiohttp.ClientSession(trust_env=True,
+                                     timeout=AIOHTTP_TIMEOUT) as session:
+        payload = {
+            "model": request_func_input.model_name \
+                if request_func_input.model_name else request_func_input.model,
+            "messages": [
+            ],
+            "temperature": 0.0,
+            "stream": True,
+            "stream_options": {
+                "include_usage": True,
+            },
+        }
+        payload["ignore_eos"] = request_func_input.ignore_eos
+        if request_func_input.extra_body:
+            payload.update(request_func_input.extra_body)
+        headers = {
+            "Content-Type": "application/json",
+            "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
+        }
+
+        output_list = []
+        for turn_id, prompt in enumerate(request_func_input.prompt):
+            output = RequestFuncOutput()
+            output.prompt_len = request_func_input.prompt_len[turn_id]
+
+            payload["messages"].append({"role": "user", "content": prompt})
+            payload["max_tokens"] = request_func_input.output_len[turn_id]
+
+            generated_text = ""
+            ttft = 0.0
+            st = time.perf_counter()
+            most_recent_timestamp = st
+            try:
+                async with session.post(url=api_url, json=payload,
+                                        headers=headers) as response:
+                    if response.status == 200:
+                        async for chunk_bytes in response.content:
+                            chunk_bytes = chunk_bytes.strip()
+                            if not chunk_bytes:
+                                continue
+
+                            chunk = chunk_bytes.decode("utf-8").removeprefix(
+                                "data: ")
+                            if chunk != "[DONE]":
+                                timestamp = time.perf_counter()
+                                data = json.loads(chunk)
+
+                                if choices := data.get("choices"):
+                                    content = choices[0]["delta"].get("content")
+                                    # First token
+                                    if ttft == 0.0:
+                                        ttft = timestamp - st
+                                        output.ttft = ttft
+
+                                    # Decoding phase
+                                    else:
+                                        output.itl.append(timestamp -
+                                                        most_recent_timestamp)
+
+                                    generated_text += content or ""
+                                elif usage := data.get("usage"):
+                                    output.output_tokens = usage.get(
+                                        "completion_tokens")
+
+                                most_recent_timestamp = timestamp
+
+                        output.generated_text = generated_text
+                        output.success = True
+                        output.latency = most_recent_timestamp - st
+                    else:
+                        output.error = response.reason or ""
+                        output.success = False
+                        break
+            except Exception:
+                output.success = False
+                exc_info = sys.exc_info()
+                output.error = "".join(traceback.format_exception(*exc_info))
+                break
+            payload["messages"].append({"role": "assistant", "content": generated_text})
+
+            output_list.append(output)
+            if turn_id != len(request_func_input.prompt) - 1:
+                await asyncio.sleep(turn_interval_time)
+
+    if pbar:
+        pbar.update(1)
+    return output_list

 def get_model(pretrained_model_name_or_path: str) -> str:
     if os.getenv("VLLM_USE_MODELSCOPE", "False").lower() == "true":
@@ -619,6 +721,7 @@ ASYNC_REQUEST_FUNCS = {
     "scalellm": async_request_openai_completions,
     "sglang": async_request_openai_completions,
     "llama.cpp": async_request_openai_completions,
+    "openai-chat-multiturns": async_request_openai_chat_completions_multiturns,
 }

 OPENAI_COMPATIBLE_BACKENDS = [
diff --git a/benchmarks/benchmark_dataset.py b/benchmarks/benchmark_dataset.py
index 1ad6cef7a..9178528d0 100644
--- a/benchmarks/benchmark_dataset.py
+++ b/benchmarks/benchmark_dataset.py
@@ -49,9 +49,9 @@ class SampleRequest:
     Represents a single inference request for benchmarking.
     """

-    prompt: Union[str, Any]
-    prompt_len: int
-    expected_output_len: int
+    prompt: Union[str, list[str], Any]
+    prompt_len: Union[int, list[int]]
+    expected_output_len: Union[int, list[int]]
     multi_modal_data: Optional[Union[MultiModalDataDict, dict]] = None
     lora_request: Optional[LoRARequest] = None

@@ -617,6 +617,108 @@ class SonnetDataset(BenchmarkDataset):
                 )
         return samples

+
+# -----------------------------------------------------------------------------
+# ShareGPT Multiturn Dataset Implementation
+# -----------------------------------------------------------------------------
+
+
+class ShareGPTMultiTurnsDataset(BenchmarkDataset):
+    def __init__(self, min_num_turns: int = 2, **kwargs) -> None:
+        super().__init__(**kwargs)
+        self.load_data(min_num_turns)
+
+    def load_data(self, min_num_turns: int) -> None:
+        if self.dataset_path is None:
+            raise ValueError("dataset_path must be provided for loading data.")
+
+        with open(self.dataset_path, encoding="utf-8") as f:
+            self.data = json.load(f)
+        # Filter entries with at least two conversation turns.
+        new_data = []
+        for entry in self.data:
+            if "conversations" in entry:
+                while len(entry["conversations"]) > 0 and entry["conversations"][0]['from'] != 'human':
+                    entry["conversations"].pop(0)
+                if len(entry["conversations"]) % 2 != 0:
+                    entry["conversations"].pop(-1)
+                if len(entry["conversations"]) >= 2 * min_num_turns:
+                    new_data.append(entry)
+        self.data = new_data
+        random.seed(self.random_seed)
+        random.shuffle(self.data)
+
+    def sample(
+        self,
+        tokenizer: PreTrainedTokenizerBase,
+        num_requests: int,
+        lora_path: Optional[str] = None,
+        max_loras: Optional[int] = None,
+        output_len: Optional[int] = None,
+        **kwargs,
+    ) -> list:
+        samples: list = []
+        for entry in self.data:
+            if len(samples) >= num_requests:
+                break
+
+            prompt_list = [d["value"] for d in entry["conversations"][::2]]
+            completion_list = [d["value"] for d in entry["conversations"][1::2]]
+            # prompt, completion = (
+            #     entry["conversations"][0]["value"],
+            #     entry["conversations"][1]["value"],
+            # )
+
+            lora_request, tokenizer = self.get_random_lora_request(
+                tokenizer=tokenizer, max_loras=max_loras, lora_path=lora_path)
+
+
+            prompt_ids_list = []
+            completion_ids_list = []
+            prompt_len_list = []
+            new_output_len_list = []
+            history_len = 0
+            for turn_id in range(len(prompt_list)):
+                try:
+                    prompt_ids = tokenizer(prompt_list[turn_id]).input_ids
+                    completion_ids = tokenizer(completion_list[turn_id]).input_ids
+                except:
+                    print(entry)
+                    raise
+                prompt_len = len(prompt_ids) + history_len
+                new_output_len = len(completion_ids) if output_len is None else output_len
+                if not is_valid_sequence(
+                                prompt_len,
+                                new_output_len,
+                                min_len=4,
+                                max_prompt_len=4096,
+                                max_total_len=8192,
+                                skip_min_output_len_check=output_len
+                                is not None):
+                    turn_id -= 1
+                    break
+                prompt_ids_list.append(prompt_ids)
+                completion_ids_list.append(completion_ids)
+                prompt_len_list.append(prompt_len)
+                new_output_len_list.append(new_output_len)
+                history_len += prompt_len
+                history_len += new_output_len
+
+            if turn_id <= 0:
+                continue
+
+            prompt_list = prompt_list[:turn_id+1]
+
+            samples.append(
+                SampleRequest(
+                    prompt=prompt_list,
+                    prompt_len=prompt_len_list,
+                    expected_output_len=new_output_len_list,
+                    lora_request=lora_request,
+                ))
+        self.maybe_oversample_requests(samples, num_requests)
+        return samples
+

 # -----------------------------------------------------------------------------
 # BurstGPT Dataset Implementation
diff --git a/benchmarks/benchmark_serving.py b/benchmarks/benchmark_serving.py
index c597fb106..74e157927 100644
--- a/benchmarks/benchmark_serving.py
+++ b/benchmarks/benchmark_serving.py
@@ -71,6 +71,7 @@ from benchmark_dataset import (
     ShareGPTDataset,
     SonnetDataset,
     VisionArenaDataset,
+    ShareGPTMultiTurnsDataset,
 )
 from benchmark_utils import convert_to_pytorch_benchmark_format, write_to_json
 from vllm.benchmarks.serve import get_request
@@ -142,7 +143,7 @@ def calculate_metrics(
                     ).input_ids
                 )
             actual_output_lens.append(output_len)
-            total_input += input_requests[i].prompt_len
+            total_input += outputs[i].prompt_len
             tpot = 0
             if output_len > 1:
                 latency_minus_ttft = outputs[i].latency - outputs[i].ttft
@@ -278,6 +279,9 @@ async def benchmark(
     )

     test_output = await request_func(request_func_input=test_input)
+    if backend == "openai-chat-multiturns":
+        print("test_output ", test_output)
+        test_output = test_output[-1]
     if not test_output.success:
         raise ValueError(
             "Initial test run failed - Please make sure benchmark arguments "
@@ -394,6 +398,8 @@ async def benchmark(
         task = limited_request_func(request_func_input=request_func_input, pbar=pbar)
         tasks.append(asyncio.create_task(task))
     outputs: list[RequestFuncOutput] = await asyncio.gather(*tasks)
+    if backend == "openai-chat-multiturns":
+        outputs = [o for sub_o in outputs for o in sub_o]

     if profile:
         print("Stopping profiler...")
@@ -748,6 +754,15 @@ def main(args: argparse.Namespace):
                 num_requests=args.num_prompts,
                 output_len=args.sharegpt_output_len,
             ),
+            "sharegpt_multiturns":
+            lambda: ShareGPTMultiTurnsDataset(
+                min_num_turns=4,
+                random_seed=args.seed,
+                dataset_path=args.dataset_path).sample(
+                    tokenizer=tokenizer,
+                    num_requests=args.num_prompts,
+                    output_len=args.sharegpt_output_len,
+                ),
             "burstgpt": lambda: BurstGPTDataset(
                 random_seed=args.seed, dataset_path=args.dataset_path
             ).sample(tokenizer=tokenizer, num_requests=args.num_prompts),
@@ -930,7 +945,7 @@ def create_argument_parser():
         "--dataset-name",
         type=str,
         default="sharegpt",
-        choices=["sharegpt", "burstgpt", "sonnet", "random", "hf", "custom"],
+        choices=["sharegpt", "burstgpt", "sonnet", "random", "hf", "sharegpt_multiturns"],
         help="Name of the dataset to benchmark on.",
     )
     parser.add_argument(
diff --git a/benchmarks/flexkv_benchmark/container b/benchmarks/flexkv_benchmark/container
new file mode 100644
index 000000000..cfc3b5bac
--- /dev/null
+++ b/benchmarks/flexkv_benchmark/container
@@ -0,0 +1,12 @@
+docker run --shm-size=50g --ipc=host --network host -it --gpus all -v /home/zichengm/FlexKV:/workspace -e GLOO_SOCKET_IFNAME=eno1 --entrypoint /bin/bash vllm/vllm-openai:v0.10.0
+VLLM_USE_PRECOMPILED=1 pip install -e .
+apt update && apt install liburing-dev
+> vllm.log 2>&1 &
+export GLOO_SOCKET_IFNAME=eno1
+nohup bash run_flexkv_server.sh > kvserver.log 2>&1 &
+nohup bash serving_vllm.sh 2 > vllm.log 2>&1 &
+bash multiturn_benchmark.sh
+
+gdb -q -ex "set pagination off" -ex "set confirm off" -ex "set env PYTHONFAULTHANDLER=1" -ex "handle SIGPIPE noprint nostop pass" -ex "handle SIGBUS stop print" -ex "run" --args python3 examples/run_server.py --model-path Qwen/Qwen3-8B --tp-size 1 --dp-size 1 --block-size 16 --num-cpu-blocks 8192 --server-recv-port ipc:///tmp/tmpe0x8_0gq
+
+cpu block num = cpu memory size / layer_num / 2 / token_per_block / num_heads / head_size / sizeof(data_type)
\ No newline at end of file
diff --git a/benchmarks/flexkv_benchmark/lmcache_config.yaml b/benchmarks/flexkv_benchmark/lmcache_config.yaml
new file mode 100644
index 000000000..8016df5b6
--- /dev/null
+++ b/benchmarks/flexkv_benchmark/lmcache_config.yaml
@@ -0,0 +1,6 @@
+# Basic configurations
+chunk_size: 16
+
+# CPU offloading configurations
+local_cpu: true
+max_local_cpu_size: 32
\ No newline at end of file
diff --git a/benchmarks/flexkv_benchmark/multiturn_benchmark.sh b/benchmarks/flexkv_benchmark/multiturn_benchmark.sh
new file mode 100644
index 000000000..4a15ca771
--- /dev/null
+++ b/benchmarks/flexkv_benchmark/multiturn_benchmark.sh
@@ -0,0 +1,17 @@
+current_time=$(date +"%Y-%m-%d-%H:%M:%S")
+for workers in 128; do
+    concurrency_multiplier=4
+    if [ $workers -gt 128 ]; then
+        concurrency_multiplier=2
+    fi
+    python3 ../benchmark_serving.py \
+        --backend openai-chat-multiturns \
+        --model Qwen/Qwen3-8B \
+        --dataset-name sharegpt_multiturns \
+        --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json \
+        --num-prompts $((workers*concurrency_multiplier)) \
+        --max-concurrency $workers \
+        --host  0.0.0.0 \
+        --port 12599 \
+        --endpoint /v1/chat/completions 2>&1
+done
\ No newline at end of file
diff --git a/benchmarks/flexkv_benchmark/run_flexkv_server.sh b/benchmarks/flexkv_benchmark/run_flexkv_server.sh
new file mode 100644
index 000000000..56b38fa01
--- /dev/null
+++ b/benchmarks/flexkv_benchmark/run_flexkv_server.sh
@@ -0,0 +1,15 @@
+MODEL_PATH=Qwen/Qwen3-8B
+
+CMD="python3 examples/run_server.py \
+    --model-path $MODEL_PATH \
+    --tp-size 1 \
+    --dp-size 1 \
+    --block-size 16 \
+    --num-cpu-blocks 7282 \
+    --server-recv-port ipc:///tmp/tmpe0x8_0gq \
+    "
+echo
+echo
+
+echo $CMD
+eval $CMD
\ No newline at end of file
diff --git a/benchmarks/flexkv_benchmark/serving_vllm.sh b/benchmarks/flexkv_benchmark/serving_vllm.sh
new file mode 100644
index 000000000..18dee4732
--- /dev/null
+++ b/benchmarks/flexkv_benchmark/serving_vllm.sh
@@ -0,0 +1,79 @@
+#!/bin/bash
+# vLLM服务启动脚本
+# 使用方法: ./serving_vllm.sh <type>
+# type选项:
+#   0: 无前缀缓存
+#   1: GPU前缀缓存
+#   2: FlexKV
+#   3: LMCache (需要先配置LMCACHE_CONFIG_FILE环境变量)
+
+MODEL_PATH=Qwen/Qwen3-8B
+
+type=${1}
+
+if [[ $type = 0 ]]; then
+    # no prefix cache
+    prefix_args="--no-enable-prefix-caching"
+    use_lmcache=false
+elif [[ $type = 1 ]]; then
+    # gpu prefix cache
+    prefix_args=""
+    use_lmcache=false
+elif [[ $type = 2 ]]; then
+    # flexkv
+    prefix_args=""
+    export ENABLE_FLEXKV="true"
+    export FLEXKV_SERVER_RECV_PORT="ipc:///tmp/tmpe0x8_0gq"
+    use_lmcache=false
+elif [[ $type = 3 ]]; then
+    # lmcache
+    prefix_args=""
+    use_lmcache=true
+    export LMCACHE_CONFIG_FILE="./lmcache_config.yaml"
+else
+    echo "ERROR: Unknown running type [$type]"
+    exit -1
+fi
+
+# nccl envs
+export GLOO_SOCKET_IFNAME=eno1
+export NCCL_SOCKET_IFNAME=eno1
+export NCCL_IB_GID_INDEX=3
+export NCCL_IB_DISABLE=0
+export NCCL_NET_GDR_LEVEL=2
+export NCCL_IB_QPS_PER_CONNECTION=4
+export NCCL_IB_TC=160
+export NCCL_IB_TIMEOUT=22
+export NCCL_PXN_DISABLE=0
+
+if [[ $use_lmcache = true ]]; then
+    # 使用vllm serve命令和LMCache
+    CMD="python3 -m vllm.entrypoints.openai.api_server --model $MODEL_PATH \
+        --port=12599 \
+        --tensor-parallel-size=1 \
+        --data-parallel-size=1 \
+        --pipeline-parallel-size=1 \
+        --max-model-len=8192 \
+        --max-num-seqs=256 \
+        --gpu-memory-utilization 0.4 \
+        --max-num-batched-tokens 8192 \
+        --kv-transfer-config '{\"kv_connector\":\"LMCacheConnectorV1\",\"kv_role\":\"kv_both\"}' \
+        $prefix_args"
+else
+    # 使用原有的api_server启动方式
+    CMD="python3 -m vllm.entrypoints.openai.api_server --model $MODEL_PATH \
+        --port=12599 \
+        --tensor-parallel-size=1 \
+        --data-parallel-size=1 \
+        --pipeline-parallel-size=1 \
+        --max-model-len=8192 \
+        --max-num-seqs=256 \
+        --gpu-memory-utilization 0.4 \
+        --max-num-batched-tokens 8192 \
+        $prefix_args"
+fi
+echo
+echo
+
+echo $CMD
+eval $CMD
\ No newline at end of file
diff --git a/examples/offline_inference/prefix_caching_flexkv.py b/examples/offline_inference/prefix_caching_flexkv.py
new file mode 100644
index 000000000..6ff17dfca
--- /dev/null
+++ b/examples/offline_inference/prefix_caching_flexkv.py
@@ -0,0 +1,123 @@
+# SPDX-License-Identifier: Apache-2.0
+import os
+
+from vllm import LLM, SamplingParams
+from vllm.distributed import cleanup_dist_env_and_memory
+
+# NOTE: This is just a running example. For benchmarking purpose,
+# please see benchmarks/benchmark_prefix_caching.py
+
+os.environ["ENABLE_FLEXKV"] = "true"
+os.environ["FLEXKV_SERVER_RECV_PORT"] = "ipc:///tmp/tmpe0x8_0gq"
+
+# Common prefix.
+prefix = (
+    "You are an expert school principal, skilled in effectively managing "
+    "faculty and staff. Draft 10-15 questions for a potential first grade "
+    "Head Teacher for my K-12, all-girls', independent school that emphasizes "
+    "community, joyful discovery, and life-long learning. The candidate is "
+    "coming in for a first-round panel interview for a 8th grade Math "
+    "teaching role. They have 5 years of previous teaching experience "
+    "as an assistant teacher at a co-ed, public school with experience "
+    "in middle school math teaching. Based on these information, fulfill "
+    "the following paragraph: ")
+
+# Sample prompts.
+prompts = [
+    "Hello, my name is",
+    "The president of the United States is",
+    "The capital of France is",
+    "The future of AI is",
+]
+
+generating_prompts = [prefix + prompt for prompt in prompts]
+
+# Create a sampling params object.
+sampling_params = SamplingParams(temperature=0.0)
+
+def main():
+    # Create an LLM without prefix caching as a baseline.
+    regular_llm = LLM(model="facebook/opt-125m",
+                      enable_prefix_caching=False,
+                      gpu_memory_utilization=0.4)
+
+    print("Results without `enable_prefix_caching`")
+
+    # ruff: noqa: E501
+    # Generate texts from the prompts. The output is a list of RequestOutput objects
+    # that contain the prompt, generated text, and other information.
+    outputs = regular_llm.generate(generating_prompts, sampling_params)
+
+    regular_generated_texts = []
+    # Print the outputs.
+    print("-" * 50)
+    for output in outputs:
+        prompt = output.prompt
+        generated_text = output.outputs[0].text
+        regular_generated_texts.append(generated_text)
+        print(f"Prompt: {prompt!r}\nGenerated text: {generated_text!r}")
+        print("-" * 50)
+
+    # Destroy the LLM object and free up the GPU memory.
+    del regular_llm
+    cleanup_dist_env_and_memory()
+
+    # Create an LLM with prefix caching enabled.
+    prefix_cached_llm = LLM(model="facebook/opt-125m",
+                            enable_prefix_caching=True,
+                            gpu_memory_utilization=0.4)
+
+    # Warmup so that the shared prompt's KV cache is computed.
+    prefix_cached_llm.generate(generating_prompts[0], sampling_params)
+
+    # Generate with prefix caching.
+    outputs = prefix_cached_llm.generate(generating_prompts, sampling_params)
+
+    print("Results with `enable_prefix_caching`")
+
+    cached_generated_texts = []
+    # Print the outputs. You should see the same outputs as before.
+    print("-" * 50)
+    for output in outputs:
+        prompt = output.prompt
+        generated_text = output.outputs[0].text
+        cached_generated_texts.append(generated_text)
+        print(f"Prompt: {prompt!r}\nGenerated text: {generated_text!r}")
+        print("-" * 50)
+
+    # Compare the results and display the speedup
+    generated_same = all([
+        regular_generated_texts[i] == cached_generated_texts[i]
+        for i in range(len(prompts))
+    ])
+    print(f"Generated answers are the same: {generated_same}")
+
+    # reset prefix cache to use flexkv
+    prefix_cached_llm.reset_prefix_cache()
+
+    # Generate with prefix caching.
+    outputs = prefix_cached_llm.generate(generating_prompts, sampling_params)
+
+    print("Results with `flexkv`")
+
+    flexkv_generated_texts = []
+    # Print the outputs. You should see the same outputs as before.
+    print("-" * 50)
+    for output in outputs:
+        prompt = output.prompt
+        generated_text = output.outputs[0].text
+        flexkv_generated_texts.append(generated_text)
+        print(f"Prompt: {prompt!r}\nGenerated text: {generated_text!r}")
+        print("-" * 50)
+
+    # Compare the results and display the speedup
+    generated_same = all([
+        regular_generated_texts[i] == flexkv_generated_texts[i]
+        for i in range(len(prompts))
+    ])
+    print(f"Generated answers are the same: {generated_same}")
+
+
+
+if __name__ == "__main__":
+    main()
diff --git a/vllm/distributed/flexkv_extension/__init__.py b/vllm/distributed/flexkv_extension/__init__.py
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/distributed/flexkv_extension/client.py b/vllm/distributed/flexkv_extension/client.py
new file mode 100644
index 000000000..478683fa9
--- /dev/null
+++ b/vllm/distributed/flexkv_extension/client.py
@@ -0,0 +1,101 @@
+import torch
+from typing import Optional
+
+from flexkv.server.client import KVDPClient, KVTPClient
+from flexkv.common.storage import KVCacheLayout, KVCacheLayoutType
+from flexkv.common.config import ModelConfig
+from vllm.distributed.flexkv_extension.config import FlexKVConfig
+from vllm.v1.kv_cache_interface import KVCacheConfig, KVCacheSpec
+from vllm.logger import init_logger
+
+logger = init_logger(__name__)
+
+
+class FlexKVDPClient:
+    def __init__(
+        self,
+        flexkv_config: FlexKVConfig
+    ):
+        self.flexkv_config = flexkv_config
+        self.server_recv_port = flexkv_config.server_recv_port
+        self.tp_size = flexkv_config.tp_size
+        self.model_config = ModelConfig(
+            num_layers=flexkv_config.num_layers,
+            num_kv_heads=flexkv_config.num_kv_heads,
+            head_size=flexkv_config.head_size,
+            use_mla=flexkv_config.use_mla,
+            dtype=flexkv_config.dtype,
+            tp_size=flexkv_config.tp_size,
+        )
+
+        logger.info(f"start init FlexKVDPClient to {self.server_recv_port}")
+        self.dp_client = KVDPClient(self.server_recv_port, self.model_config)
+        logger.info(f"finish init FlexKVDPClient")
+
+    def put_async(
+        self,
+        token_ids: torch.Tensor,
+        slot_mapping: torch.Tensor,
+        token_mask: Optional[torch.Tensor] = None,
+    ) -> int:
+        " return task_id "
+        return self.dp_client.put_async(token_ids, slot_mapping, token_mask)
+
+    def get_async(
+        self,
+        token_ids: torch.Tensor,
+        slot_mapping: torch.Tensor,
+        token_mask: Optional[torch.Tensor] = None,
+    ) -> int:
+        " return task_id "
+        return self.dp_client.get_async(token_ids, slot_mapping, token_mask)
+
+    def wait(
+        self,
+        wait_task_ids: list[int],
+    ) -> dict[int, torch.Tensor]:
+        return self.dp_client.wait(wait_task_ids)
+
+    def try_wait(
+        self,
+        wait_task_ids: list[int],
+    ) -> dict[int, Optional[torch.Tensor]]:
+        # print("--------------------------------")
+        # print(f"[FlexKVDPClient] About to call dp_client.try_wait with {wait_task_ids}")
+        try:
+            result = self.dp_client.try_wait(wait_task_ids)
+            # print(f"[FlexKVDPClient] dp_client.try_wait returned: {result}")
+            return result
+        except Exception as e:
+            # print(f"[FlexKVDPClient ERROR] Exception calling dp_client.try_wait: {e}")
+            import traceback
+            traceback.print_exc()
+            return {}
+
+
+class FlexKVTPClient:
+    def __init__(
+        self,
+        flexkv_config: FlexKVConfig,
+        dp_client_id: int,
+        tp_rank: int,
+        device_id: int,
+        gpu_blocks: list[torch.Tensor],
+        kv_shape: tuple[int],
+    ):
+        logger.info(f"start init FlexKVTPClient to {flexkv_config.server_recv_port}")
+        self.tp_client = KVTPClient(flexkv_config.server_recv_port, dp_client_id, device_id, tp_rank)
+        logger.info(f"finish init FlexKVTPClient")
+        gpu_layout = KVCacheLayout(
+            type=KVCacheLayoutType.LAYERFIRST,
+            num_layer=flexkv_config.num_layers,
+            num_block=flexkv_config.num_blocks,
+            tokens_per_block=flexkv_config.block_size,
+            num_head=flexkv_config.num_kv_heads,
+            head_size=flexkv_config.head_size,
+            is_mla=flexkv_config.use_mla,
+        )
+        logger.info(f"start register FlexKVTPClient")
+        self.tp_client.register_to_server(gpu_blocks, gpu_layout)
+
+        logger.info(f"finish register FlexKVTPClient")
\ No newline at end of file
diff --git a/vllm/distributed/flexkv_extension/config.py b/vllm/distributed/flexkv_extension/config.py
new file mode 100644
index 000000000..f2724e712
--- /dev/null
+++ b/vllm/distributed/flexkv_extension/config.py
@@ -0,0 +1,45 @@
+from dataclasses import dataclass
+import json
+import os
+import torch
+from vllm.v1.kv_cache_interface import KVCacheConfig, FullAttentionSpec
+from vllm.logger import init_logger
+
+logger = init_logger(__name__)
+
+
+@dataclass
+class FlexKVConfig:
+    enable_flexkv: bool
+    server_recv_port: str
+    num_blocks: int = None
+    block_size: int = None
+    num_layers: int = None
+    num_kv_heads: int = None
+    head_size: int = None
+    dtype: torch.dtype = None
+    use_mla: bool = False
+    tp_size: int = 1
+
+    @classmethod
+    def from_env(cls) -> 'FlexKVConfig':
+        enable_flexkv = (os.getenv('ENABLE_FLEXKV', "false").lower() == "true")
+        server_recv_port = os.getenv('FLEXKV_SERVER_RECV_PORT', "")
+
+        return cls(enable_flexkv=enable_flexkv,
+                   server_recv_port=server_recv_port)
+
+    def post_init(
+        self,
+        kv_cache_config: KVCacheConfig,
+        tp_size: int
+        ):
+        self.num_blocks = kv_cache_config.num_blocks
+        self.num_layers = len(kv_cache_config.kv_cache_groups)
+        kv_cache_spec: FullAttentionSpec = kv_cache_config.kv_cache_groups[0].kv_cache_spec
+        self.block_size = kv_cache_spec.block_size
+        self.num_kv_heads = kv_cache_spec.num_kv_heads
+        self.head_size = kv_cache_spec.head_size
+        self.dtype = kv_cache_spec.dtype
+        self.use_mla = kv_cache_spec.use_mla
+        self.tp_size = tp_size
\ No newline at end of file
diff --git a/vllm/logger.py b/vllm/logger.py
index 69aaf4390..fe426f420 100644
--- a/vllm/logger.py
+++ b/vllm/logger.py
@@ -21,7 +21,7 @@ VLLM_LOGGING_CONFIG_PATH = envs.VLLM_LOGGING_CONFIG_PATH
 VLLM_LOGGING_LEVEL = envs.VLLM_LOGGING_LEVEL
 VLLM_LOGGING_PREFIX = envs.VLLM_LOGGING_PREFIX

-_FORMAT = (f"{VLLM_LOGGING_PREFIX}%(levelname)s %(asctime)s "
+_FORMAT = (f"{VLLM_LOGGING_PREFIX}%(levelname)s %(asctime)s.%(msecs)03d "
            "[%(filename)s:%(lineno)d] %(message)s")
 _DATE_FORMAT = "%m-%d %H:%M:%S"

diff --git a/vllm/v1/core/kv_cache_utils.py b/vllm/v1/core/kv_cache_utils.py
index 5b0218640..aa590eb6f 100644
--- a/vllm/v1/core/kv_cache_utils.py
+++ b/vllm/v1/core/kv_cache_utils.py
@@ -87,8 +87,9 @@ class PrefixCachingMetrics:
         self.aggregated_requests = 0
         self.aggregated_query_total = 0
         self.aggregated_query_hit = 0
+        self.aggregated_query_flexkv_hit = 0
         # A deque of (requests, queries, hits) for the most recent requests.
-        self.query_queue: deque[tuple[int, int, int]] = deque()
+        self.query_queue: deque[tuple[int, int, int, int]] = deque()

     def observe(self, stats: PrefixCacheStats):
         """Observe the prefix caching for a set of requests.
@@ -108,14 +109,15 @@ class PrefixCachingMetrics:
             self.reset()

         # Update the metrics.
-        self.query_queue.append((stats.requests, stats.queries, stats.hits))
+        self.query_queue.append((stats.requests, stats.queries, stats.hits, stats.flexkv_hits))
         self.aggregated_requests += stats.requests
         self.aggregated_query_total += stats.queries
         self.aggregated_query_hit += stats.hits
+        self.aggregated_query_flexkv_hit += stats.flexkv_hits

         # Remove the oldest stats if the number of requests exceeds.
         if self.aggregated_requests > self.max_recent_requests:
-            old_requests, old_queries, old_hits = self.query_queue.popleft()
+            old_requests, old_queries, old_hits, _ = self.query_queue.popleft()
             self.aggregated_requests -= old_requests
             self.aggregated_query_total -= old_queries
             self.aggregated_query_hit -= old_hits
@@ -125,6 +127,7 @@ class PrefixCachingMetrics:
         self.aggregated_requests = 0
         self.aggregated_query_total = 0
         self.aggregated_query_hit = 0
+        self.aggregated_query_flexkv_hit = 0
         self.query_queue.clear()

     @property
@@ -133,6 +136,13 @@ class PrefixCachingMetrics:
         if self.aggregated_query_total == 0:
             return 0.0
         return self.aggregated_query_hit / self.aggregated_query_total
+
+    @property
+    def flexkv_hit_rate(self) -> float:
+        """Calculate the hit rate for the past N requests."""
+        if self.aggregated_query_total == 0:
+            return 0.0
+        return self.aggregated_query_flexkv_hit / self.aggregated_query_total


 @dataclass
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 446f98034..b465c4cf1 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -5,6 +5,7 @@ from __future__ import annotations

 import itertools
 import time
+import torch
 from collections import defaultdict
 from collections.abc import Iterable
 from typing import Any, Optional, Union
@@ -34,6 +35,9 @@ from vllm.v1.outputs import ModelRunnerOutput
 from vllm.v1.request import Request, RequestStatus
 from vllm.v1.spec_decode.metrics import SpecDecodingStats
 from vllm.v1.structured_output import StructuredOutputManager
+# flexkv
+from vllm.utils import cdiv
+from vllm.distributed.flexkv_extension.config import FlexKVConfig

 logger = init_logger(__name__)

@@ -162,6 +166,23 @@ class Scheduler(SchedulerInterface):
         )
         self.use_pp = self.parallel_config.pipeline_parallel_size > 1

+        # flexkv
+        self.enable_flexkv = False
+        self.flexkv_client = None
+        # task_id -> Request
+        self.load_kv_tasks: dict[int, Request] = {}
+        # task_id -> Request
+        self.offload_kv_tasks: dict[int, Request] = {}
+        # request_id -> time info
+        self.flexkv_timer: dict[str, dict[str, float]] = {}
+
+
+    def init_flexkv(self, flexkv_config: FlexKVConfig) -> int:
+        self.enable_flexkv = True
+        from vllm.distributed.flexkv_extension.client import FlexKVDPClient
+        self.flexkv_client = FlexKVDPClient(flexkv_config)
+        return self.flexkv_client.dp_client.dp_client_id
+
     def schedule(self) -> SchedulerOutput:
         # NOTE(woosuk) on the scheduling algorithm:
         # There's no "decoding phase" nor "prefill phase" in the scheduler.
@@ -174,6 +195,13 @@ class Scheduler(SchedulerInterface):
         # chunked prefills, prefix caching, speculative decoding,
         # and the "jump decoding" optimization in the future.

+        # flexkv
+        if self.enable_flexkv:
+            # aviod busy loop
+            if self.get_num_unfinished_requests() == 0:
+                time.sleep(0.01)
+            self.check_offload_kv_tasks()
+
         scheduled_new_reqs: list[Request] = []
         scheduled_resumed_reqs: list[Request] = []
         scheduled_running_reqs: list[Request] = []
@@ -448,6 +476,27 @@ class Scheduler(SchedulerInterface):
                 if new_blocks is None:
                     # The request cannot be scheduled.
                     break
+
+                if self.enable_flexkv and num_new_tokens > self.block_size and request.status == RequestStatus.WAITING:
+                    # don't match the last block
+                    num_new_blocks_to_get = cdiv(num_new_tokens, self.block_size)-1
+                    num_new_tokens_to_match = num_new_blocks_to_get*self.block_size
+                    num_tokens_to_get = num_computed_tokens + num_new_tokens_to_match
+                    blocks_ids_to_get = [block.block_id for block in new_blocks.blocks[0][:num_new_blocks_to_get]]
+                    slot_mapping = torch.tensor(blocks_ids_to_get).repeat_interleave(self.block_size)*self.block_size
+                    token_mask_to_get = torch.ones(num_tokens_to_get, dtype=torch.bool)
+                    token_mask_to_get[:num_computed_tokens] = False
+                    t_async_get_start = time.monotonic()
+                    task_id = self.flexkv_client.get_async(
+                        token_ids=torch.tensor(request.all_token_ids[:num_tokens_to_get]),
+                        slot_mapping=slot_mapping,
+                        token_mask=token_mask_to_get)
+                    t_async_get_return = time.monotonic()
+
+                    self.load_kv_tasks[task_id] = request
+                    self.flexkv_timer[request.request_id] = {}
+                    self.flexkv_timer[request.request_id]['get_async_start'] = t_async_get_start
+                    self.flexkv_timer[request.request_id]['get_async_return'] = t_async_get_return

                 # KVTransfer: the connector uses this info to determine
                 # if a load is needed. Note that
@@ -505,6 +554,31 @@ class Scheduler(SchedulerInterface):
                     for i in encoder_inputs_to_schedule:
                         self.encoder_cache_manager.allocate(request, i)
                     encoder_budget = new_encoder_budget
+                        # batch wait
+
+            # batch wait
+            if self.enable_flexkv:
+                if len(self.load_kv_tasks) != 0:
+                    task_ids = list(self.load_kv_tasks.keys())
+                    print(f"[DEBUG] scheduler wait for {task_ids}")
+                    results = self.flexkv_client.wait(task_ids)
+                    print(f"[DEBUG] scheduler wait result: {results}")
+                    t_async_get_end = time.monotonic()
+                    for task_id, task_result in results.items():
+                        request = self.load_kv_tasks.pop(task_id)
+                        t_get_async_start = self.flexkv_timer[request.request_id]["get_async_start"]
+                        t_get_async_return =  self.flexkv_timer[request.request_id]["get_async_return"]
+                        match_length = task_result.sum().item()
+                        self.flexkv_timer.pop(request.request_id)
+                        logger.info(
+                            f"[FlexKV] req: {request.request_id}, task: {task_id}, "
+                            f"get {match_length} tokens cost {(t_async_get_end-t_get_async_start)*1000:.2f} ms, "
+                            f"get_async() api cost {(t_get_async_return-t_get_async_start)*1000:.2f} ms")
+
+                        token_budget += match_length
+                        num_scheduled_tokens[request.request_id] -= match_length
+                        request.num_computed_tokens += match_length
+                        self.kv_cache_manager.prefix_cache_stats.flexkv_hits += (match_length//self.block_size)

         # Put back any skipped requests at the head of the waiting queue
         if skipped_waiting_requests:
@@ -1016,11 +1090,49 @@ class Scheduler(SchedulerInterface):
         if self.finished_req_ids_dict is not None:
             self.finished_req_ids_dict[request.client_index].add(request_id)

-        if not delay_free_blocks:
-            self._free_blocks(request)
+        # flexkv: offload BEFORE freeing blocks to preserve req_to_blocks info
+        if self.enable_flexkv:
+            self._offload_kv(request)
+        else:
+            if not delay_free_blocks:
+                self._free_blocks(request)
+            # else:
+            #     self._free_block(request)
+

         return kv_xfer_params

+    def _free_block(self, request: Request) -> None:
+        self.kv_cache_manager.free(request)
+        self.kv_cache_manager.free_block_hashes(request)
+        del self.requests[request.request_id]
+
+    def _offload_kv(self, request: Request):
+        # print(f"single_type_managers: {self.kv_cache_manager.coordinator.single_type_managers}")
+        # print(f"req_to_blocks: {self.kv_cache_manager.coordinator.single_type_managers[0].req_to_blocks}")
+        req_blocks = self.kv_cache_manager.coordinator.single_type_managers[0].req_to_blocks.get(request.request_id, [])
+        req_token_ids = torch.tensor(request.all_token_ids[:-1])
+        req_block_ids = torch.tensor([block.block_id for block in req_blocks])
+
+        # Debug information for empty req_blocks
+        # if len(req_blocks) == 0:
+            # print(f"WARNING: Empty req_blocks for request {request.request_id}")
+            # print(f"  request.all_token_ids length: {len(request.all_token_ids)}")
+            # print(f"  req_token_ids length: {len(req_token_ids)}")
+            # print(f"  req_to_blocks keys: {list(self.kv_cache_manager.coordinator.single_type_managers[0].req_to_blocks.keys())}")
+
+        slot_mapping = req_block_ids.repeat_interleave(self.block_size)[:len(req_token_ids)] * self.block_size
+
+        # Additional debug info
+        # print(f"FlexKV _offload_kv: req_id={request.request_id}, "
+            #   f"blocks={len(req_blocks)}, tokens={len(req_token_ids)}, slots={len(slot_mapping)}")
+
+        self.flexkv_timer[request.request_id] = {}
+        self.flexkv_timer[request.request_id]["put_async_start"] = time.monotonic()
+        task_id = self.flexkv_client.put_async(token_ids=req_token_ids, slot_mapping=slot_mapping)
+        self.offload_kv_tasks[task_id] = request
+        self.flexkv_timer[request.request_id]["put_async_return"] = time.monotonic()
+
     def _free_blocks(self, request: Request):
         assert request.is_finished()
         self.kv_cache_manager.free(request)
@@ -1068,7 +1180,27 @@ class Scheduler(SchedulerInterface):
             num_draft_tokens=num_draft_tokens,
             num_accepted_tokens=num_accepted_tokens)
         return spec_decoding_stats
-
+
+    def check_offload_kv_tasks(self):
+        if len(self.offload_kv_tasks) == 0:
+            return
+        logger.info(f"check_offload_kv_tasks")
+        task_ids = list(self.offload_kv_tasks.keys())
+        results = self.flexkv_client.try_wait(task_ids)
+        # logger.info(f"results {results}")
+        t_async_put_end = time.monotonic()
+        for task_id, task_result in results.items():
+            if task_result is not None:
+                request = self.offload_kv_tasks.pop(task_id)
+                t_put_async_start = self.flexkv_timer[request.request_id]["put_async_start"]
+                t_put_async_return =  self.flexkv_timer[request.request_id]["put_async_return"]
+                self.flexkv_timer.pop(request.request_id)
+                logger.info(
+                    f"[FlexKV] req: {request.request_id}, task: {task_id}, "
+                    f"put {sum(task_result).item()} tokens cost {(t_async_put_end-t_put_async_start)*1000:.2f} ms, "
+                    f"put_async() api cost {(t_put_async_return-t_put_async_start)*1000:.2f} ms")
+                self._free_block(request)
+
     def shutdown(self) -> None:
         if self.kv_event_publisher:
             self.kv_event_publisher.shutdown()
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 7779b559c..2d17908ea 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -46,6 +46,8 @@ from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder
 from vllm.v1.structured_output import StructuredOutputManager
 from vllm.version import __version__ as VLLM_VERSION

+from vllm.distributed.flexkv_extension.config import FlexKVConfig
+
 logger = init_logger(__name__)

 POLLING_TIMEOUT_S = 2.5
@@ -118,6 +120,8 @@ class EngineCore:
             log_stats=self.log_stats,
         )

+        self.init_flexkv(vllm_config, kv_cache_config)
+
         # Setup MM Input Mapper.
         self.mm_input_cache_server = MirroredProcessingCache(
             vllm_config.model_config)
@@ -194,6 +198,23 @@ class EngineCore:
                      "warmup model) took %.2f seconds"), elapsed)
         return num_gpu_blocks, num_cpu_blocks, scheduler_kv_cache_config

+
+    def init_flexkv(
+        self,
+        taco_llm_config: VllmConfig,
+        kv_cache_config: KVCacheConfig
+    ):
+        self.scheduler: V1Scheduler
+        if taco_llm_config.cache_config.enable_prefix_caching:
+            flexkv_config = FlexKVConfig.from_env()
+            if flexkv_config.enable_flexkv:
+                flexkv_config.post_init(
+                    kv_cache_config=kv_cache_config,
+                    tp_size=taco_llm_config.parallel_config.tensor_parallel_size,
+                )
+                dp_client_id = self.scheduler.init_flexkv(flexkv_config)
+                self.model_executor.init_flexkv(flexkv_config, dp_client_id)
+
     def add_request(self, request: EngineCoreRequest):
         """Add request to the scheduler."""
         if pooling_params := request.pooling_params:
diff --git a/vllm/v1/executor/abstract.py b/vllm/v1/executor/abstract.py
index 50b9634a4..3d7bdd4c8 100644
--- a/vllm/v1/executor/abstract.py
+++ b/vllm/v1/executor/abstract.py
@@ -15,7 +15,7 @@ from vllm.executor.uniproc_executor import (  # noqa
     UniProcExecutor as UniProcExecutorV0)
 from vllm.v1.kv_cache_interface import KVCacheConfig, KVCacheSpec
 from vllm.v1.outputs import ModelRunnerOutput
-
+from vllm.distributed.flexkv_extension.config import FlexKVConfig
 FailureCallback = Callable[[], None]


@@ -88,6 +88,10 @@ class Executor(ExecutorBase):
                                      args=(scheduler_output, ))
         return output[0]

+    def init_flexkv(self, flexkv_config: FlexKVConfig, dp_client_id: int):
+        self.collective_rpc("init_flexkv",
+                            args=(flexkv_config, dp_client_id, ))
+
     @property
     def max_concurrent_batches(self) -> int:
         return 1
diff --git a/vllm/v1/metrics/loggers.py b/vllm/v1/metrics/loggers.py
index 7f2556bab..e7fb79486 100644
--- a/vllm/v1/metrics/loggers.py
+++ b/vllm/v1/metrics/loggers.py
@@ -125,7 +125,8 @@ class LoggingStatLogger(StatLoggerBase):
             "Avg generation throughput: %.1f tokens/s, "
             "Running: %d reqs, Waiting: %d reqs, "
             "GPU KV cache usage: %.1f%%, "
-            "Prefix cache hit rate: %.1f%%",
+            "Prefix cache hit rate: %.1f%%, "
+            "FlexKV hit rate: %.1f%%",
             self.engine_index,
             prompt_throughput,
             generation_throughput,
@@ -133,6 +134,7 @@ class LoggingStatLogger(StatLoggerBase):
             scheduler_stats.num_waiting_reqs,
             scheduler_stats.kv_cache_usage * 100,
             self.prefix_caching_metrics.hit_rate * 100,
+            self.prefix_caching_metrics.flexkv_hit_rate * 100,
         )
         self.spec_decoding_logging.log(log_fn=log_fn)

diff --git a/vllm/v1/metrics/stats.py b/vllm/v1/metrics/stats.py
index 1eb10ccb6..1073aa571 100644
--- a/vllm/v1/metrics/stats.py
+++ b/vllm/v1/metrics/stats.py
@@ -24,7 +24,8 @@ class PrefixCacheStats:
     queries: int = 0
     # The number of hits in these requests.
     hits: int = 0
-
+    # flexkv
+    flexkv_hits: int = 0

 @dataclass
 class SchedulerStats:
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index a5bf197ba..d10265d0c 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -2494,6 +2494,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         ) == 0, "Attention backends are already initialized"
         for i, kv_cache_group_spec in enumerate(
                 kv_cache_config.kv_cache_groups):
+            print("init attn backend ", i)
             kv_cache_spec = kv_cache_group_spec.kv_cache_spec
             if isinstance(kv_cache_spec, AttentionSpec):
                 attn_backend_i = get_attn_backend(
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index 522946351..31a3bed13 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -18,7 +18,7 @@ from vllm.distributed import (ensure_model_parallel_initialized,
                               set_custom_all_reduce)
 from vllm.distributed.kv_transfer import (ensure_kv_transfer_initialized,
                                           has_kv_transfer_group)
-from vllm.distributed.parallel_state import get_pp_group, get_tp_group
+from vllm.distributed.parallel_state import get_pp_group, get_tp_group, get_tensor_model_parallel_rank
 from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
 from vllm.model_executor import set_random_seed
@@ -33,6 +33,10 @@ from vllm.v1.utils import report_usage_stats
 from vllm.v1.worker.gpu_model_runner import GPUModelRunner
 from vllm.v1.worker.worker_base import WorkerBase

+# flexkv
+from vllm.distributed.flexkv_extension.config import FlexKVConfig
+from vllm.distributed.flexkv_extension.client import FlexKVTPClient
+
 logger = init_logger(__name__)

 if TYPE_CHECKING:
@@ -556,6 +560,23 @@ class Worker(WorkerBase):
             max_size=max_size,
         )

+    def init_flexkv(
+        self,
+        flexkv_config: FlexKVConfig,
+        dp_client_id: int,
+    ) -> None:
+        from vllm.distributed.flexkv_extension.client import FlexKVTPClient
+        layer_kv_shape = self.model_runner.attn_backends[0].get_kv_cache_shape(
+                        flexkv_config.num_blocks, flexkv_config.block_size,
+                        flexkv_config.num_kv_heads, flexkv_config.head_size)
+        kv_shape = (flexkv_config.num_layers, *layer_kv_shape)
+        self.flexkv_client = FlexKVTPClient(flexkv_config=flexkv_config,
+                                            dp_client_id=dp_client_id,
+                                            tp_rank=get_tensor_model_parallel_rank(),
+                                            device_id=self.device.index,
+                                            gpu_blocks=self.model_runner.kv_caches,
+                                            kv_shape=kv_shape)
+
     def save_tensorized_model(
         self,
         tensorizer_config: "TensorizerConfig",
