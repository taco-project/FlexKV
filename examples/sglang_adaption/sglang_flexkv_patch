diff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py
index 401f702a..c7d174f6 100644
--- a/python/sglang/bench_serving.py
+++ b/python/sglang/bench_serving.py
@@ -1077,14 +1077,42 @@ def sample_sharegpt_requests(
         for data in dataset
         if len(data.get("conversations", data.get("conversation", []))) >= 2
     ]
-    # Only keep the first two turns of each conversation.
-    dataset = [
-        (
-            data.get("conversations", data.get("conversation", []))[0]["value"],
-            data.get("conversations", data.get("conversation", []))[1]["value"],
-        )
-        for data in dataset
-    ]
+
+    # # Only keep the first two turns of each conversation.
+    # dataset = [
+    #     (
+    #         data.get("conversations", data.get("conversation", []))[0]["value"],
+    #         data.get("conversations", data.get("conversation", []))[1]["value"],
+    #     )
+    #     for data in dataset
+    # ]
+
+    # Process multi-turn conversations: construct full conversation history as prompt
+    # and use the last assistant response as completion
+    processed_dataset = []
+    for data in dataset:
+        conversations = data.get("conversations", data.get("conversation", []))
+
+        # Build conversation history (all turns except the last assistant response)
+        conversation_history = []
+        last_assistant_response = None
+
+        for i, conv in enumerate(conversations):
+            if conv.get("from") == "assistant" and i == len(conversations) - 1:
+                # This is the last assistant response, use as completion
+                last_assistant_response = conv["value"]
+                break
+            else:
+                # Add to conversation history
+                role = "User" if conv.get("from") == "user" else "Assistant"
+                conversation_history.append(f"{role}: {conv['value']}")
+
+        # Only include if we have both conversation history and a final assistant response
+        if conversation_history and last_assistant_response:
+            prompt = "\n\n".join(conversation_history) + "\n\nAssistant:"
+            processed_dataset.append((prompt, last_assistant_response))
+
+    dataset = processed_dataset

     # Shuffle the dataset.
     random.shuffle(dataset)
diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index a246534c..d700938d 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -722,6 +722,27 @@ class Scheduler(
                     tp_group=self.tp_group,
                     eviction_policy=server_args.radix_eviction_policy,
                 )
+            elif server_args.enable_flexkv:
+                from sglang.srt.mem_cache.storage.flexkv.flexkv_radix_cache import (
+                    FlexKVRadixCache,
+                )
+
+                self.tree_cache = FlexKVRadixCache(
+                    req_to_token_pool=self.req_to_token_pool,
+                    token_to_kv_pool_allocator=self.token_to_kv_pool_allocator,
+                    page_size=self.page_size,
+                    disable=server_args.disable_radix_cache,
+                    enable_kv_cache_events=self.enable_kv_cache_events,
+                    model_config=self.model_config,
+                    tp_size=self.tp_size,
+                    rank=self.tp_rank,
+                    tp_group=(
+                        self.attn_tp_cpu_group
+                        if self.server_args.enable_dp_attention
+                        else self.tp_cpu_group
+                    ), # self.tp_group,
+                    eviction_policy=server_args.radix_eviction_policy,
+                )
             else:
                 self.tree_cache = RadixCache(
                     req_to_token_pool=self.req_to_token_pool,
@@ -1539,7 +1560,7 @@ class Scheduler(
             self.handle_embedding_request(tokenized_req)

     def self_check_during_idle(self):
-        self.check_memory()
+        # self.check_memory()
         self.check_tree_cache()
         self.new_token_ratio = self.init_new_token_ratio
         self.maybe_sleep_on_idle()
diff --git a/python/sglang/srt/mem_cache/storage/flexkv/flexkv_radix_cache.py b/python/sglang/srt/mem_cache/storage/flexkv/flexkv_radix_cache.py
new file mode 100644
index 00000000..76212afb
--- /dev/null
+++ b/python/sglang/srt/mem_cache/storage/flexkv/flexkv_radix_cache.py
@@ -0,0 +1,608 @@
+import time
+import torch
+import logging
+import threading
+import numpy as np
+from typing import List, Optional, Dict, Callable
+
+from sglang.srt.configs.model_config import ModelConfig, AttentionArch
+from sglang.srt.managers.schedule_batch import Req
+from sglang.srt.mem_cache.allocator import BaseTokenToKVPoolAllocator
+from sglang.srt.mem_cache.base_prefix_cache import MatchResult
+from sglang.srt.mem_cache.memory_pool import ReqToTokenPool
+from sglang.srt.mem_cache.radix_cache import RadixCache, TreeNode
+from sglang.srt.utils import broadcast_pyobj
+
+try:
+    from flexkv.kvmanager import KVManager
+    from flexkv.common.config import ModelConfig as FlexKVModelConfig, CacheConfig
+    from flexkv.server.client import KVTPClient
+    from flexkv.common.storage import KVCacheLayout, KVCacheLayoutType
+    from flexkv.common.request import KVResponseStatus
+except ImportError as e:
+    raise RuntimeError(
+        "FlexKV is not installed. Please install it."
+    ) from e
+
+logger = logging.getLogger(__name__)
+
+
+class FlexKVConnector:
+    """
+    Manages KV cache operations through FlexKV's distributed cache system.
+    """
+
+    def __init__(
+        self,
+        sgl_config: ModelConfig,
+        page_size: int,
+        tp_size: int,
+        tp_rank: int,
+        k_pool: torch.Tensor,
+        v_pool: torch.Tensor,
+        tp_group: Optional[torch.distributed.ProcessGroup] = None,
+    ):
+        self.sgl_config = sgl_config
+        self.tp_size = tp_size
+        self.rank = tp_rank
+        self.k_pool = k_pool
+        self.v_pool = v_pool
+        self.tp_group = tp_group
+
+        self.model_config = FlexKVModelConfig(
+            num_layers=sgl_config.num_hidden_layers,
+            num_kv_heads=sgl_config.get_num_kv_heads(tp_size),
+            head_size=sgl_config.head_dim,
+            use_mla=sgl_config.attention_arch == AttentionArch.MLA,
+            dtype=sgl_config.dtype,
+            tp_size=tp_size,
+        )
+
+        self.cache_config = CacheConfig(
+            tokens_per_block=page_size,
+            enable_cpu=True,
+            enable_ssd=False,
+            enable_remote=False,
+            num_cpu_blocks=600000, # 407778, # 174762, # 116508, # if no bug, larger is better
+        )
+
+        # import uuid
+        # self.gpu_register_port = f"ipc:///tmp/flexkv_gpu_{uuid.uuid4().hex[:8]}"
+        # self.server_recv_port = f"ipc:///tmp/flexkv_srv_{uuid.uuid4().hex[:8]}"
+        self.gpu_register_port = f"ipc:///tmp/flexkv_gpu_12345678"
+        self.server_recv_port = f"ipc:///tmp/flexkv_srv_87654321"
+
+        if self.rank == 0:
+            self.kv_manager = KVManager(
+                model_config=self.model_config,
+                cache_config=self.cache_config,
+                gpu_register_port=self.gpu_register_port,
+                server_recv_port=self.server_recv_port
+            )
+            self.kv_manager.start()
+
+        self.tp_client = KVTPClient(self.gpu_register_port, 0, self.rank)
+        self.register_to_server(self.k_pool, self.v_pool)
+
+        # Track task_id -> req_id mapping of store_async (only maintained on rank 0),
+        self.inflight_taskid2reqid: Dict[int, int] = {} if self.rank == 0 else {}
+
+        if self.rank == 0:
+            while not self.kv_manager.is_ready():
+                time.sleep(3)
+                logger.info("waiting for flexkv to be ready")
+            logger.info("flexkv is ready")
+
+        logger.info(f"FlexKV connector initialized for rank {self.rank}")
+
+    def chunk_size(self) -> int:
+        """Return the chunk size used by FlexKV."""
+        return self.cache_config.tokens_per_block
+
+    def start_load_kv(
+        self,
+        token_ids: List[int],
+        slot_mapping: torch.Tensor,
+        token_mask: Optional[torch.Tensor] = None,
+        inflight_reqid2node: Optional[Dict[int, TreeNode]] = None,
+        dec_lock_ref_fn: Optional[Callable[[TreeNode], None]] = None,
+    ) -> tuple[int, Optional[torch.Tensor]]:
+        """
+        Start loading KV cache from FlexKV storage.
+
+        Args:
+            token_ids: List of token IDs to load
+            slot_mapping: Tensor mapping for slots
+            token_mask: Optional mask indicating which tokens to load from FlexKV
+
+        Returns:
+            Tuple of (number of tokens loaded, loaded slot IDs tensor)
+        """
+
+        if self.rank == 0:
+            token_ids_np = np.array(token_ids, dtype=np.int64) # isinstance(token_ids, list):
+
+            task_id, matched_mask = self.kv_manager.get_match(
+                token_ids=token_ids_np,
+                token_mask=token_mask,
+            )
+
+            # try_wait any in-flight store tasks to get completed req_ids
+            inflight_task_ids = list(self.inflight_taskid2reqid.keys())
+            completed_dict = self.kv_manager.try_wait(task_ids=inflight_task_ids) # task_id -> KVResponse
+            completed_req_ids = []
+            for task_id in completed_dict:
+                if task_id in self.inflight_taskid2reqid:
+                    req_id = self.inflight_taskid2reqid[task_id]
+                    del self.inflight_taskid2reqid[task_id]
+                    completed_req_ids.append(req_id)
+            logger.info(f"[flexkv][inside start_load_kv] nums of completed reqs: {len(completed_req_ids)}")
+
+            if matched_mask.sum() > 0:
+                filtered_slot_mapping = slot_mapping[matched_mask]
+                slot_mapping_cpu = filtered_slot_mapping.cpu() if filtered_slot_mapping.is_cuda else filtered_slot_mapping
+                self.kv_manager.launch(task_ids=[task_id], slot_mappings=[slot_mapping_cpu])
+                response = self.kv_manager.wait([task_id])
+
+                if task_id in response and response[task_id].status == KVResponseStatus.SUCCESS:
+                    num_loaded = matched_mask.sum().item()
+                    requested_tokens = token_mask.sum().item() if token_mask is not None else len(token_ids)
+                    logger.debug(f"FlexKV loaded {num_loaded}/{requested_tokens} tokens from cache")
+
+                    loaded_slot_ids = filtered_slot_mapping
+                    # Update broadcast_data with actual values
+                    broadcast_data = {
+                        'num_loaded': num_loaded,
+                        'loaded_slot_ids': loaded_slot_ids,
+                        'completed_req_ids': completed_req_ids,
+                    }
+            else:
+                broadcast_data = {
+                    'num_loaded': 0,
+                    'loaded_slot_ids': None,
+                    'completed_req_ids': completed_req_ids,
+                }
+        else:
+            broadcast_data = None
+
+        if self.tp_group is not None:
+            broadcast_data = broadcast_pyobj([broadcast_data], self.rank, self.tp_group, src=0)[0]
+
+            # release locks for completed reqs
+            for req_id in broadcast_data['completed_req_ids']:
+                if req_id in inflight_reqid2node:
+                    node = inflight_reqid2node[req_id]
+                    dec_lock_ref_fn(node)
+                    del inflight_reqid2node[req_id]
+
+            return broadcast_data['num_loaded'], broadcast_data['loaded_slot_ids']
+        else:
+            # For single process case, return values from rank 0's broadcast_data
+            if self.rank == 0 and broadcast_data is not None:
+                return broadcast_data['num_loaded'], broadcast_data['loaded_slot_ids']
+            else:
+                # Fallback for non-rank0 or when no data was loaded
+                return 0, None
+
+    def store_kv_async(self, token_ids: List[int], kv_indices: torch.Tensor, req_id: int) -> int:
+        """
+        Store KV cache to FlexKV storage asynchronously.
+
+        Args:
+            token_ids: List of token IDs to store
+            kv_indices: Tensor of KV indices
+            req_id: Request ID for tracking
+
+        Returns:
+            task_id for tracking the async operation
+        """
+        try:
+            # Only tp0 performs actual FlexKV operations, other ranks return dummy task_id
+            if self.rank == 0:
+                token_ids_np = np.array(token_ids, dtype=np.int64) # isinstance(token_ids, list)
+                assert len(token_ids) == len(kv_indices), "token_ids and kv_indices must have the same length"
+
+                task_id, unmatched_mask = self.kv_manager.put_match(
+                    token_ids=token_ids_np,
+                    token_mask=None,  # Store all tokens
+                )
+
+                if unmatched_mask.sum() > 0:
+                    filtered_kv_indices = kv_indices[unmatched_mask]
+                    slot_mapping_cpu = filtered_kv_indices.cpu() if filtered_kv_indices.is_cuda else filtered_kv_indices
+                    self.kv_manager.launch(task_ids=[task_id], slot_mappings=[slot_mapping_cpu])
+                    self.inflight_taskid2reqid[task_id] = req_id
+                    logger.debug(f"FlexKV storing {unmatched_mask.sum().item()}/{len(token_ids)} tokens to cache (async)")
+                    return task_id
+                else:
+                    logger.debug(f"All {len(token_ids)} tokens already in FlexKV cache")
+                    return -1  # No task launched
+            else:
+                # Other ranks don't perform actual operations
+                return -1
+
+        except Exception as e:
+            logger.error(f"FlexKV store_kv_async failed: {e}")
+            return -1
+
+    def wait_task(self, task_id: int, timeout: float = 20.0) -> bool:
+        """
+        Wait for a task to complete.
+
+        Args:
+            task_id: Task ID to wait for
+            timeout: Maximum time to wait in seconds
+
+        Returns:
+            True if task completed successfully, False otherwise
+        """
+        if task_id < 0:
+            return True  # No task to wait for (dummy task or no-op)
+
+        # Only tp0 has real tasks to wait for
+        if self.rank == 0:
+            try:
+                response = self.kv_manager.wait([task_id], timeout=timeout)
+                if task_id in response and response[task_id].status == KVResponseStatus.SUCCESS:
+                    logger.debug(f"FlexKV task {task_id} completed successfully")
+                    return True
+                else:
+                    logger.warning(f"FlexKV task {task_id} failed: status={response.get(task_id, {}).status if task_id in response else 'NOT_FOUND'}")
+                    return False
+            except Exception as e:
+                logger.error(f"FlexKV wait_task failed: {e}")
+                return False
+        else:
+            # Other ranks don't have real tasks, so always return success
+            return True
+
+    def register_to_server(self, k_caches: List[torch.Tensor], v_caches: List[torch.Tensor]) -> None:
+        logger.info("Start register kv_caches")
+        assert len(k_caches) == len(v_caches), "k_caches and v_caches must have the same length"
+        num_layer = len(k_caches)
+
+        # not mla
+        assert k_caches[0].ndim == 3, (
+            f"expect kv cached tensor has 3 dim but get shape={k_caches[0].shape}.")
+
+        num_layer = len(k_caches)
+        num_blocks = k_caches[0].shape[0]
+        num_kv_heads = k_caches[0].shape[1]
+        head_size = k_caches[0].shape[2]
+        gpu_layout = KVCacheLayout(
+            type=KVCacheLayoutType.LAYERWISE,
+            num_layer=num_layer,
+            num_block=num_blocks,
+            tokens_per_block=1,
+            num_head=num_kv_heads,
+            head_size=head_size,
+            is_mla=False,
+            )
+        gpu_blocks = k_caches + v_caches
+        self.tp_client.register_to_server(gpu_blocks, gpu_layout)
+        logger.info("Finish register kv_caches")
+
+    def shutdown(self) -> None:
+        """Shutdown FlexKV connection."""
+        self.kv_manager.shutdown()
+
+
+class FlexKVRadixCache(RadixCache):
+    def __init__(
+        self,
+        req_to_token_pool: ReqToTokenPool,
+        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
+        page_size: int,
+        disable: bool = False,
+        enable_kv_cache_events: bool = False,
+        model_config: Optional[ModelConfig] = None,
+        tp_size: int = 1,
+        rank: int = 0,
+        tp_group: Optional[torch.distributed.ProcessGroup] = None,
+        eviction_policy: str = "lru",
+    ):
+        # Initialize attributes needed by reset() method before calling super().__init__()
+        self.rank = rank
+        self.tp_group = tp_group
+
+        # Track req_id -> gpu last node, for all ranks
+        self.inflight_reqid2node: Dict[int, TreeNode] = {}
+        self._node_lock = threading.Lock()
+        self.sts_total_seq_len = 0
+        self.sts_gpu_cache_len = 0
+        self.sts_flexkv_cache_len = 0
+
+        # Initialize FlexKV connector before super().__init__() since reset() method may use it
+        kvcache = token_to_kv_pool_allocator.get_kvcache()
+        self.flexkv_connector = FlexKVConnector(
+            sgl_config=model_config,
+            page_size=page_size,
+            tp_size=tp_size,
+            tp_rank=rank,
+            tp_group=tp_group,
+            k_pool=getattr(
+                kvcache,
+                "k_buffer",
+                getattr(token_to_kv_pool_allocator._kvcache, "k_buffer"),
+            ),
+            v_pool=getattr(
+                kvcache,
+                "v_buffer",
+                getattr(token_to_kv_pool_allocator._kvcache, "v_buffer"),
+            ),
+        )
+
+        super().__init__(
+            req_to_token_pool=req_to_token_pool,
+            token_to_kv_pool_allocator=token_to_kv_pool_allocator,
+            page_size=page_size,
+            disable=disable,
+            enable_kv_cache_events=enable_kv_cache_events,
+            eviction_policy=eviction_policy,
+        )
+
+    def reset(self):
+        super().reset()
+        with self._node_lock:
+            # Wait for all in-flight tasks before reset
+            # Only rank 0 needs to wait, other ranks just clear their mappings
+            if self.rank == 0:
+                task_ids = list(self.flexkv_connector.inflight_taskid2reqid.keys())
+                for task_id in task_ids:
+                    self.flexkv_connector.wait_task(task_id)
+
+            # All ranks clean up their node mappings
+            for _, node in self.inflight_reqid2node.items():
+                self.dec_lock_ref(node)
+            self.inflight_reqid2node.clear()
+
+    def match_prefix(self, key: List[int], **kwargs) -> MatchResult:
+        self.sts_total_seq_len += len(key)
+        if self.disable or not key:
+            return super().match_prefix(key, **kwargs)
+
+        if self.page_size != 1:
+            aligned_len = len(key) // self.page_size * self.page_size
+            key = key[:aligned_len]
+
+        base_res = super().match_prefix(key, **kwargs)
+        value: torch.Tensor = base_res.device_indices
+        self.sts_gpu_cache_len += value.numel()
+
+        last_node: TreeNode = base_res.last_device_node
+
+        uncached_len = len(key) - value.numel()
+        if uncached_len == 0:
+            return base_res
+
+        chunk_size = self.flexkv_connector.chunk_size()
+        prefix_pad = value.numel() % chunk_size
+
+        if self.token_to_kv_pool_allocator.available_size() < uncached_len:
+            self.evict(uncached_len)
+
+        token_slots = self.token_to_kv_pool_allocator.alloc(uncached_len)
+        if token_slots is None:
+            return base_res
+
+        slot_mapping = torch.cat(
+            [
+                torch.full((value.numel(),), -1, dtype=torch.int64, device=self.device),
+                token_slots.detach().clone().to(torch.int64).to(self.device),
+            ]
+        )
+
+        token_mask = torch.zeros(len(key), dtype=torch.bool)
+        token_mask[value.numel():] = True  # Only load uncached tokens
+
+        num_retrieved, loaded_slot_ids = self.flexkv_connector.start_load_kv(
+            token_ids=key,
+            slot_mapping=slot_mapping,
+            token_mask=token_mask,
+            inflight_reqid2node=self.inflight_reqid2node,
+            dec_lock_ref_fn=self.dec_lock_ref,
+        )
+        self.sts_flexkv_cache_len += num_retrieved
+        if self.rank == 0:
+            # logger.debug("num_retrieved_tokens: %s", num_retrieved)
+            logger.info(f"[FlexKV stats] total_seq_len={self.sts_total_seq_len}, "
+            f"gpu_cache_len={self.sts_gpu_cache_len}, flexkv_cache_len={self.sts_flexkv_cache_len}, "
+            f"gpu_cache_ratio={self.sts_gpu_cache_len / self.sts_total_seq_len}, "
+            f"flexkv_cache_ratio={self.sts_flexkv_cache_len / self.sts_total_seq_len}")
+
+        if num_retrieved > prefix_pad:
+            fetched = num_retrieved - prefix_pad
+            # Free unused token slots
+            self.token_to_kv_pool_allocator.free(
+                token_slots[fetched:]
+            )
+
+            new_node = TreeNode()
+            start = value.numel()
+            end = start + fetched
+            new_node.key = key[start:end]
+            new_node.value = token_slots[:fetched]
+            new_node.parent = last_node
+            last_node.children[self.get_child_key_fn(new_node.key)] = new_node
+            last_node = new_node
+
+            value = torch.cat([value, token_slots[:fetched]])
+            self.evictable_size_ += fetched
+
+            self._record_store_event(new_node.parent)
+            self._record_store_event(new_node)
+
+            return MatchResult(
+                device_indices=value,
+                last_device_node=last_node,
+                last_host_node=last_node,
+            )
+        else:
+            logger.debug(f"FlexKV retrieved {num_retrieved} tokens but need at least {prefix_pad} for alignment, freeing all allocated slots")
+            self.token_to_kv_pool_allocator.free(token_slots)
+
+        return base_res
+
+    def cache_finished_req(self, req: Req) -> None:
+        """
+        Cache finished request:
+        1. Insert into GPU radix tree (via super())
+        2. Store to FlexKV asynchronously
+        3. Track the node to prevent eviction during transfer
+        """
+        if hasattr(req, 'is_retracted') and req.is_retracted:
+            super().cache_finished_req(req)
+            return
+
+        # Insert into GPU radix tree
+        super().cache_finished_req(req)
+
+        if req.req_pool_idx is None:
+            logger.warning("[FlexKV] req_pool_idx is None, request may have been retracted")
+            return
+
+        token_ids = (req.origin_input_ids + req.output_ids)[:-1]
+        kv_indices = self.req_to_token_pool.req_to_token[
+            req.req_pool_idx, : len(token_ids)
+        ]
+
+        new_last_node = req.last_node
+        if new_last_node is None:
+            logger.warning("[FlexKV] req.last_node is None, skipping FlexKV store")
+            return
+
+        try:
+            task_id = self.flexkv_connector.store_kv_async(
+                token_ids=token_ids,
+                kv_indices=kv_indices,
+                req_id=req.req_pool_idx,
+            )
+        except Exception as e:
+            logger.error(f"[FlexKV] Failed to store KV: {e}")
+            return
+
+        # Lock the node to prevent eviction during transfer
+        self.inc_lock_ref(new_last_node)
+
+        with self._node_lock:
+            self.inflight_reqid2node[req.req_pool_idx] = new_last_node
+
+        # logger.debug(f"[FlexKV] Locked nodes: {len(self.inflight_reqid2node)}, protected_size: {self.protected_size()}, evictable_size: {self.evictable_size()}")
+
+    def cache_unfinished_req(self, req: Req, chunked=False) -> None:
+        """Cache request when it is unfinished."""
+
+        if self.disable:
+            return
+
+        if req.req_pool_idx is None:
+            logger.warning("[FlexKV] req_pool_idx is None, request may have been retracted")
+            return
+
+        token_ids = req.fill_ids
+        kv_indices = self.req_to_token_pool.req_to_token[
+            req.req_pool_idx, : len(token_ids)
+        ]
+
+        if self.page_size != 1:
+            page_aligned_len = len(kv_indices) // self.page_size * self.page_size
+            page_aligned_kv_indices = kv_indices[:page_aligned_len].to(
+                dtype=torch.int64, copy=True
+            )
+        else:
+            page_aligned_len = len(kv_indices)
+            page_aligned_kv_indices = kv_indices.to(dtype=torch.int64, copy=True)
+        page_aligned_token_ids = token_ids[:page_aligned_len]
+
+        new_prefix_len = self.insert(
+            page_aligned_token_ids, page_aligned_kv_indices, chunked=chunked
+        )
+        self.token_to_kv_pool_allocator.free(
+            kv_indices[len(req.prefix_indices) : new_prefix_len]
+        )
+
+        new_indices, new_last_node, _, _ = super().match_prefix(page_aligned_token_ids)
+        self.req_to_token_pool.write(
+            (req.req_pool_idx, slice(len(req.prefix_indices), len(new_indices))),
+            new_indices[len(req.prefix_indices) :],
+        )
+
+        self.dec_lock_ref(req.last_node)
+        self.inc_lock_ref(new_last_node)
+
+        if self.page_size != 1:
+            req.prefix_indices = torch.cat(
+                [new_indices, kv_indices[len(new_indices) :]]
+            )
+        else:
+            req.prefix_indices = new_indices
+        req.last_node = new_last_node
+
+        try:
+            task_id = self.flexkv_connector.store_kv_async(
+                token_ids=page_aligned_token_ids,
+                kv_indices=page_aligned_kv_indices,
+                req_id=req.req_pool_idx,
+            )
+        except Exception as e:
+            logger.error(f"[FlexKV] Failed to store KV: {e}")
+            return
+
+        self.inc_lock_ref(new_last_node)
+        with self._node_lock:
+            self.inflight_reqid2node[req.req_pool_idx] = new_last_node
+        logger.info(f"[FlexKV] Locked nodes: {len(self.inflight_reqid2node)}, protected_size: {self.protected_size()}, evictable_size: {self.evictable_size()}")
+
+    def evict(self, num_tokens: int) -> None:
+        """
+        Before base eviction, wait for any outstanding stores and release locks.
+        This ensures we don't evict KV that is still being transferred to FlexKV.
+        """
+        if self.disable:
+            return
+
+        logger.info(f"[FlexKV] evict() called for {num_tokens} tokens, locked nodes: {len(self.inflight_reqid2node)}")
+
+        with self._node_lock:
+            remaining_reqids = list(self.inflight_reqid2node.keys())
+
+            if self.flexkv_connector.rank == 0:
+                task_ids = list(self.flexkv_connector.inflight_taskid2reqid.keys())
+                for task_id in task_ids:
+                    logger.debug(f"[FlexKV] Waiting for task {task_id}")
+                    self.flexkv_connector.wait_task(task_id)
+                self.flexkv_connector.inflight_taskid2reqid.clear()
+
+            for req_id in remaining_reqids:
+                node = self.inflight_reqid2node[req_id]
+                self.dec_lock_ref(node)
+            self.inflight_reqid2node.clear()
+
+        super().evict(num_tokens)
+
+    def pretty_print(self):
+        super().pretty_print()
+        try:
+            logger.debug(
+                "evictable=%d protected=%d", self.evictable_size_, self.protected_size_
+            )
+        except Exception:
+            pass
+
+
+if __name__ == "__main__":
+    cache = FlexKVRadixCache(
+        req_to_token_pool=None,
+        token_to_kv_pool_allocator=None,
+        page_size=1,
+        disable=False,
+        enable_kv_cache_events=False,
+        model_config=None,
+        tp_size=1,
+        rank=0,
+        tp_group=None,
+    )
+    cache.insert([1, 2, 3], torch.tensor([10, 11, 12], dtype=torch.int64))
+    cache.insert([1, 2, 3, 4], torch.tensor([10, 11, 12, 13], dtype=torch.int64))
+    cache.pretty_print()
+
\ No newline at end of file
diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py
index 3459d67e..f7c520b1 100644
--- a/python/sglang/srt/server_args.py
+++ b/python/sglang/srt/server_args.py
@@ -329,6 +329,9 @@ class ServerArgs:
     # LMCache
     enable_lmcache: bool = False

+    # FlexKV
+    enable_flexkv: bool = False
+
     # Double Sparsity
     enable_double_sparsity: bool = False
     ds_channel_config_path: Optional[str] = None
@@ -1956,6 +1959,13 @@ class ServerArgs:
             help="Using LMCache as an alternative hierarchical cache solution",
         )

+        # FlexKV
+        parser.add_argument(
+            "--enable-flexkv",
+            action="store_true",
+            help="Using FlexKV as an alternative distributed cache solution",
+        )
+
         # Double Sparsity
         parser.add_argument(
             "--enable-double-sparsity",
