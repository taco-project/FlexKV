From 840161b405b729226c5331da65ae04b3457bb6b8 Mon Sep 17 00:00:00 2001
From: scutizhang <scutizhang@tencent.com>
Date: Wed, 22 Oct 2025 14:23:31 +0800
Subject: [PATCH] 1. add flexkv support

2. support flexkv + cuda graph
---
 .../tensorrt_llm/batch_manager/llmRequest.h   | 22 +++++++++++++++++++
 .../batch_manager/kvCacheManager.cpp          |  1 +
 .../nanobind/batch_manager/bindings.cpp       |  2 ++
 .../pybind/batch_manager/bindings.cpp         |  2 ++
 .../_torch/pyexecutor/kv_cache_connector.py   |  6 +++++
 tensorrt_llm/_torch/pyexecutor/py_executor.py | 12 ++++++++--
 .../_torch/pyexecutor/py_executor_creator.py  | 10 +++++++--
 7 files changed, 51 insertions(+), 4 deletions(-)

diff --git a/cpp/include/tensorrt_llm/batch_manager/llmRequest.h b/cpp/include/tensorrt_llm/batch_manager/llmRequest.h
index f069e3ac7..e74e6a01a 100644
--- a/cpp/include/tensorrt_llm/batch_manager/llmRequest.h
+++ b/cpp/include/tensorrt_llm/batch_manager/llmRequest.h
@@ -1054,6 +1054,16 @@ public:
         return mUseDraftModel ? mPrepopulatedPromptLenDraft : mPrepopulatedPromptLenTarget;
     }
 
+    [[nodiscard]] SizeType32 getNumConnectorMatchedTokens() const
+    {
+        return mNumConnectorMatchedTokens;
+    }
+
+    void setNumConnectorMatchedTokens(SizeType32 numConnectorMatchedTokens)
+    {
+        mNumConnectorMatchedTokens = numConnectorMatchedTokens;
+    }
+
     void setPrepopulatedPromptLen(SizeType32 prepopulatedPromptLen, SizeType32 kvTokensPerBlock)
     {
         // Add debug log for prepopulatedPromptLen
@@ -1658,6 +1668,15 @@ public:
             [](auto reason) { return reason == executor::FinishReason::kLENGTH; });
     }
 
+    [[nodiscard]] bool isFinishedNormal() const noexcept
+    {
+        return std::all_of(mFinishReasons.begin(), mFinishReasons.end(),
+            [](auto reason) { 
+                return  reason == executor::FinishReason::kEND_ID || \
+                        reason == executor::FinishReason::kSTOP_WORDS || \
+                        reason == executor::FinishReason::kLENGTH; });
+    }
+
     [[nodiscard]] bool isTimedOut() const
     {
         if (!mAllottedTimeMs.has_value())
@@ -1906,6 +1925,9 @@ protected:
     SizeType32 mPrepopulatedPromptLenTarget{0};
     SizeType32 mPrepopulatedPromptLenDraft{0};
 
+    // Number of tokens matched by KV cache connector for block reuse.
+    SizeType32 mNumConnectorMatchedTokens{0};
+
     SizeType32 mMaxSentTokenLen;
 
     std::optional<TensorPtr> mEmbeddingBias{std::nullopt};
diff --git a/cpp/tensorrt_llm/batch_manager/kvCacheManager.cpp b/cpp/tensorrt_llm/batch_manager/kvCacheManager.cpp
index 175b52577..98e1e6b40 100644
--- a/cpp/tensorrt_llm/batch_manager/kvCacheManager.cpp
+++ b/cpp/tensorrt_llm/batch_manager/kvCacheManager.cpp
@@ -1202,6 +1202,7 @@ void WindowBlockManager::addSequence(
     if (mKvCacheConnectorManager && !llmRequest.isDummyRequest())
     {
         numConnectorMatchedTokens = mKvCacheConnectorManager->getNumNewMatchedTokens(llmRequest, prepopulatedPromptLen);
+        llmRequest.setNumConnectorMatchedTokens(numConnectorMatchedTokens);
     }
 
     llmRequest.setPrepopulatedPromptLen(prepopulatedPromptLen + numConnectorMatchedTokens, getTokensPerBlock());
diff --git a/cpp/tensorrt_llm/nanobind/batch_manager/bindings.cpp b/cpp/tensorrt_llm/nanobind/batch_manager/bindings.cpp
index c170ca810..7fd5d5afe 100644
--- a/cpp/tensorrt_llm/nanobind/batch_manager/bindings.cpp
+++ b/cpp/tensorrt_llm/nanobind/batch_manager/bindings.cpp
@@ -159,9 +159,11 @@ void initBindings(nb::module_& m)
         .def("set_finished_reason", &GenLlmReq::setFinishedReason, nb::arg("finish_reason"), nb::arg("beam"))
         .def_prop_ro("is_finished", &GenLlmReq::isFinished)
         .def_prop_ro("is_finished_due_to_length", &GenLlmReq::isFinishedDueToLength)
+        .def_prop_ro("is_finished_normal", &GenLlmReq::isFinishedNormal)
         .def_prop_rw(
             "context_current_position", &GenLlmReq::getContextCurrentPosition, &GenLlmReq::setContextCurrentPosition)
         .def_prop_ro("prepopulated_prompt_len", &GenLlmReq::getPrepopulatedPromptLen)
+        .def_prop_ro("num_connector_matched_tokens", &GenLlmReq::getNumConnectorMatchedTokens)
         .def_prop_rw("guided_decoding_params", &GenLlmReq::getGuidedDecodingParams, &GenLlmReq::setGuidedDecodingParams)
         .def_prop_ro("context_phase_params", &GenLlmReq::getContextPhaseParams)
         .def_prop_ro("is_context_only_request", &GenLlmReq::isContextOnlyRequest)
diff --git a/cpp/tensorrt_llm/pybind/batch_manager/bindings.cpp b/cpp/tensorrt_llm/pybind/batch_manager/bindings.cpp
index 53c9ec7ef..c4be230d2 100644
--- a/cpp/tensorrt_llm/pybind/batch_manager/bindings.cpp
+++ b/cpp/tensorrt_llm/pybind/batch_manager/bindings.cpp
@@ -164,9 +164,11 @@ void initBindings(pybind11::module_& m)
         .def("set_finished_reason", &GenLlmReq::setFinishedReason, py::arg("finish_reason"), py::arg("beam"))
         .def_property_readonly("is_finished", &GenLlmReq::isFinished)
         .def_property_readonly("is_finished_due_to_length", &GenLlmReq::isFinishedDueToLength)
+        .def_property_readonly("is_finished_normal", &GenLlmReq::isFinishedNormal)
         .def_property(
             "context_current_position", &GenLlmReq::getContextCurrentPosition, &GenLlmReq::setContextCurrentPosition)
         .def_property_readonly("prepopulated_prompt_len", &GenLlmReq::getPrepopulatedPromptLen)
+        .def_property_readonly("num_connector_matched_tokens", &GenLlmReq::getNumConnectorMatchedTokens)
         .def_property(
             "guided_decoding_params", &GenLlmReq::getGuidedDecodingParams, &GenLlmReq::setGuidedDecodingParams)
         .def_property_readonly("context_phase_params", &GenLlmReq::getContextPhaseParams)
diff --git a/tensorrt_llm/_torch/pyexecutor/kv_cache_connector.py b/tensorrt_llm/_torch/pyexecutor/kv_cache_connector.py
index 5e8bf6dfa..ec22d269e 100644
--- a/tensorrt_llm/_torch/pyexecutor/kv_cache_connector.py
+++ b/tensorrt_llm/_torch/pyexecutor/kv_cache_connector.py
@@ -392,6 +392,12 @@ class KvCacheConnectorManager(KvCacheConnectorManagerCpp):
 
     def get_num_new_matched_tokens(self, request: LlmRequest,
                                    num_computed_tokens: int) -> int:
+        """ Called in C++:  KVCacheManager::addSequence
+        if (mKvCacheConnectorManager && !llmRequest.isDummyRequest())
+        {
+            numConnectorMatchedTokens = mKvCacheConnectorManager->getNumNewMatchedTokens(llmRequest, prepopulatedPromptLen);
+        }
+        """
         num_tokens, load_kv_async = self._run_on_leader(
             lambda: self.scheduler.get_num_new_matched_tokens(
                 request, num_computed_tokens))
diff --git a/tensorrt_llm/_torch/pyexecutor/py_executor.py b/tensorrt_llm/_torch/pyexecutor/py_executor.py
index 4b3315560..56b2450e2 100644
--- a/tensorrt_llm/_torch/pyexecutor/py_executor.py
+++ b/tensorrt_llm/_torch/pyexecutor/py_executor.py
@@ -282,6 +282,8 @@ class PyExecutor:
         if start_worker:
             self.start_worker()
 
+        self._wait_for_flexkv_manager()
+
     def _maybe_init_kv_connector_manager(self):
         if self.kv_connector_manager is not None:
             if self.kv_cache_transceiver is not None:
@@ -309,6 +311,12 @@ class PyExecutor:
                         self.kv_connector_manager.layer_pre_hook)
                     module.register_forward_hook(
                         self.kv_connector_manager.layer_post_hook)
+    
+    def _wait_for_flexkv_manager(self):
+        if self.kv_connector_manager is not None and self.dist.rank == 0:
+            while not self.kv_connector_manager.scheduler.is_ready():
+                time.sleep(0.1)
+            logger.info("FlexKV manager is ready")
 
     def _event_loop_wrapper(self):
         try:
@@ -518,7 +526,7 @@ class PyExecutor:
                 if prev_device_step_time is None:
                     prev_device_step_time = "N/A"  # Handle first iteration
                 else:
-                    prev_device_step_time = f"{prev_device_step_time}ms"
+                    prev_device_step_time = f"{prev_device_step_time:.3f} ms"
                 host_step_time = (end_time - start_time) * 1000  # milliseconds
                 formatted_timestamp = datetime.datetime.now().strftime(
                     "%Y-%m-%d %H:%M:%S")
@@ -528,7 +536,7 @@ class PyExecutor:
                     f"rank = {self.dist.rank}, "
                     f"currank_total_requests = {self.executor_request_queue.num_fetch_requests_cur_rank}/"
                     f"{self.executor_request_queue.num_fetch_requests}, "
-                    f"host_step_time = {host_step_time}ms, "
+                    f"host_step_time = {host_step_time:.3f} ms, "
                     f"prev_device_step_time = {prev_device_step_time}, "
                     f"timestamp = {formatted_timestamp}, "
                     f"num_scheduled_requests: {self.num_scheduled_requests}, "
diff --git a/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py b/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py
index e824ee02d..63e0848ca 100644
--- a/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py
+++ b/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py
@@ -1,6 +1,7 @@
 import copy
 import enum
 import importlib
+import os
 from concurrent.futures import ThreadPoolExecutor
 from contextlib import contextmanager
 from dataclasses import dataclass
@@ -202,6 +203,7 @@ def _get_mapping(executor_config: ExecutorConfig) -> Mapping:
                           tp_size=tensorrt_llm.mpi_world_size(),
                           gpus_per_node=tensorrt_llm.default_gpus_per_node(),
                           rank=tensorrt_llm.mpi_rank())
+        executor_config.mapping = mapping
     else:
         mapping = copy.deepcopy(executor_config.mapping)
         mapping.rank = tensorrt_llm.mpi_rank()
@@ -388,8 +390,12 @@ def create_py_executor(
             f"Initializing kv connector with config: {kv_connector_config}")
 
         if pytorch_backend_config.use_cuda_graph:
-            raise NotImplementedError(
-                "CUDA graphs are not supported with KV connector hooks.")
+            use_flexkv = os.getenv("TENSORRT_LLM_USE_FLEXKV", "0")
+            if use_flexkv == "0":
+                raise NotImplementedError(
+                    "CUDA graphs are not supported with KV connector hooks.")
+            else:
+                logger.info("Using FlexKV for KV connector")
 
         if executor_config.scheduler_config.capacity_scheduler_policy != CapacitySchedulerPolicy.GUARANTEED_NO_EVICT:
             raise NotImplementedError(
-- 
2.43.0

