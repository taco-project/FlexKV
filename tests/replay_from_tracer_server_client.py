#!/usr/bin/env python3
"""
Replay script for FlexKV trace files in server-client mode

This script reads trace files generated by FlexKV tracer and replays the entire
workflow in a server-client architecture including:
1. Starting KVServer process
2. Starting KVDPClient and KVTPClient processes
3. Replaying requests through client-server communication
4. Coordinating wait operations across processes

Usage:
    python replay_from_tracer_server_client.py --trace-file ./flexkv_trace.log --verbose
"""

import os
# Set environment variables BEFORE importing torch to avoid UCX issues
os.environ['UCX_LOG_LEVEL'] = 'error'
os.environ['UCX_WARN_UNUSED_ENV_VARS'] = 'n'
os.environ['NCCL_IB_DISABLE'] = '1'
os.environ['NCCL_P2P_DISABLE'] = '1'
os.environ['NCCL_DEBUG'] = 'WARN'
# Disable UCX transport methods that might cause issues
os.environ['UCX_TLS'] = 'dummy'  # Use dummy transport to avoid bus errors
os.environ['UCX_NET_DEVICES'] = 'lo'
os.environ['UCX_RNDV_SCHEME'] = 'put_zcopy'
os.environ['UCX_RNDV_THRESH'] = '0'
os.environ['UCX_MEMTYPE_CACHE'] = 'n'
os.environ['UCX_IB_GPU_DIRECT_RDMA'] = 'no'
# Additional UCX settings to prevent bus errors
os.environ['UCX_POSIX_USE_PROC_LINK'] = 'n'
os.environ['UCX_MM_FIFO_SIZE'] = '64'
os.environ['UCX_RC_VERBS_TL_ENABLE'] = 'n'
os.environ['UCX_UD_VERBS_TL_ENABLE'] = 'n'
os.environ['UCX_DC_VERBS_TL_ENABLE'] = 'n'
os.environ['UCX_PROTO_ENABLE'] = 'n'
os.environ['UCX_WARN_UNUSED_ENV_VARS'] = 'n'
# Disable InfiniBand completely
os.environ['UCX_IB_ENABLE'] = 'n'
# Set PyTorch to use CPU backend for multiprocessing  
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
# Disable problematic PyTorch features
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

import argparse
import json
import sys
import time
import tempfile
import signal
from multiprocessing import Process, Queue, Event
from typing import Dict, List, Optional, Any, Tuple
import torch

from flexkv.common.config import CacheConfig, ModelConfig
from flexkv.common.storage import KVCacheLayout, KVCacheLayoutType
from flexkv.common.debug import flexkv_logger
from flexkv.server.client import KVDPClient, KVTPClient
from flexkv.server.server import KVServer

# Don't set special multiprocessing method - use default like test_multiprocess.py
# import torch.multiprocessing as mp
# try:
#     mp.set_start_method('fork', force=True)  # Use fork instead of spawn
# except RuntimeError:
#     pass  # Already set

# Global variables to store configurations (like test_multiprocess.py)
global_model_config = None
global_cache_config = None
global_gpu_layout = None
global_gpu_blocks_num = 1000
global_verbose = True

def log_message(message: str):
    """Log message if verbose mode is enabled"""
    if global_verbose:
        print(f"[REPLAY] {message}")

def run_server_process(server_recv_port: str):
    """Run KVServer process"""
    log_message("Starting KVServer...")
    kvserver = KVServer(global_model_config, global_cache_config, server_recv_port)
    kvserver.run()

def run_tp_client_process(dp_client_id: int, tp_rank: int, device_id: int, server_recv_port: str):
    """Run KVTPClient process"""
    log_message(f"Starting TP client: dp_client_id={dp_client_id}, tp_rank={tp_rank}, device_id={device_id}")
    
    try:
        # Set CUDA device for this process
        if torch.cuda.is_available():
            torch.cuda.set_device(device_id)
            # Initialize CUDA context
            torch.cuda.init()
            # Clear cache
            torch.cuda.empty_cache()
            
        tp_client = KVTPClient(server_recv_port, dp_client_id, device_id, tp_rank)

        # Create GPU blocks for this TP client
        gpu_blocks = []
        for layer_id in range(global_model_config.num_layers):
            kv_dim = 2 if not global_model_config.use_mla else 1
            kv_tensor = torch.zeros(
                size=(kv_dim, global_gpu_blocks_num, global_cache_config.tokens_per_block,
                      global_model_config.num_kv_heads // global_model_config.tp_size,
                      global_model_config.head_size),
                dtype=global_model_config.dtype,
                device=f"cuda:{device_id}"
            )
            gpu_blocks.append(kv_tensor)

        # Ê≥®ÂÜåÂà∞ÊúçÂä°Âô® - ‰ΩøÁî®ÂêåÊ≠•Êé•Âè£ÔºàTPÂÆ¢Êà∑Á´ØÈÄöÂ∏∏Âú®ÂçïÁã¨ËøõÁ®ã‰∏≠ÂêåÊ≠•ËøêË°åÔºâ
        tp_client.register_to_server(gpu_blocks, global_gpu_layout)

        # Keep TP client running
        while True:
            time.sleep(1)
            
    except Exception as e:
        log_message(f"TP client error: {e}")
        import traceback
        traceback.print_exc()
        raise

def run_dp_client_process(dp_client_id: int, server_recv_port: str, request_queue: Queue, response_queue: Queue, stop_event: Event):
    """Run KVDPClient process and handle request replay"""
    log_message(f"Starting DP client: dp_client_id={dp_client_id}")
    
    try:
        # Initialize DP client
        dp_client = KVDPClient(server_recv_port, global_model_config)
        
        # Start TP client processes (following test_multiprocess.py pattern)
        tp_client_processes = []
        for tp_rank in range(global_model_config.tp_size):
            device_id = tp_rank + dp_client_id * global_model_config.tp_size
            # Use available GPU or fallback to GPU 0
            available_gpus = torch.cuda.device_count()
            if device_id >= available_gpus:
                device_id = device_id % available_gpus
                
            tp_client_process = Process(
                target=run_tp_client_process,
                args=(dp_client_id, tp_rank, device_id, server_recv_port),
                daemon=True,  # TP clients are daemon processes
            )
            tp_client_process.start()
            tp_client_processes.append(tp_client_process)

        # Wait for TP clients to register
        time.sleep(10)

        # Handle request replay
        request_id_mapping = {}
        
        while not stop_event.is_set():
            try:
                # Get request from queue with timeout (non-blocking)
                try:
                    request = request_queue.get_nowait()
                except:
                    time.sleep(0.001)  # Short sleep to prevent busy waiting
                    continue
                
                if request is None:  # Sentinel value to stop
                    break
                
                event_type = request['event_type']
                event_data = request['data']
                
                if event_type == 'request':
                    # Replay request event
                    original_request_id = event_data['request_id'] + (1+dp_client_id) * 10000000
                    request_type = event_data['request_type']
                    
                    # Convert lists back to tensors
                    token_ids = torch.tensor(event_data['token_ids'], dtype=torch.long)
                    slot_mapping = torch.tensor(event_data['slot_mapping'], dtype=torch.long)
                    token_mask = torch.tensor(event_data['token_mask'], dtype=torch.bool) if event_data['token_mask'] else None
                    layer_granularity = event_data.get('layer_granularity', -1)
                    dp_id = event_data.get('dp_id', 0)
                    
                    log_message(f"Replaying {request_type} request with {len(token_ids)} tokens")
                    
                    if request_type == "GET":
                        print(f"üîçüîçüîçGET token_ids: {token_ids[:4]}")
                        print(f"request_id: {original_request_id}, request_type: {request_type}, "
                              f"input length: {len(token_ids)}, true in mask: {token_mask.sum()}")
                        # ‰ΩøÁî®ÂêåÊ≠•Êé•Âè£
                        task_id = dp_client.get_async(
                            token_ids=token_ids,
                            slot_mapping=slot_mapping,
                            token_mask=token_mask
                        )
                    elif request_type == "PUT":
                        print(f"‚úÖ‚úÖ‚úÖPUT token_ids: {token_ids[:4]}")
                        print(f"request_id: {original_request_id}, request_type: {request_type}, "
                              f"input length: {len(token_ids)}, true in mask: {token_mask.sum()}")
                        # ‰ΩøÁî®ÂêåÊ≠•Êé•Âè£
                        start_time_inner = time.time()
                        task_id = dp_client.put_async(
                            token_ids=token_ids,
                            slot_mapping=slot_mapping,
                            token_mask=token_mask
                        )
                        end_time_inner = time.time()
                        print(f"üîçüîçüîçIN DPCLIENT. THE PUT TIME COST: {(end_time_inner - start_time_inner)*1000:.2f} ms")
                    else:
                        log_message(f"Unknown request type: {request_type}")
                        continue
                    
                    request_id_mapping[original_request_id] = task_id
                    #print(f"add request_id_mapping: {original_request_id} -> {task_id}")
                    
                    # Send response back to main process
                    response_queue.put({
                        'type': 'request_completed',
                        'original_request_id': original_request_id,
                        'task_id': task_id
                    })
                    
                elif event_type == 'wait':
                    # Replay wait event
                    original_task_ids = event_data['task_ids']
                    wait_type = event_data['wait_type']
                    layer_group_id = event_data.get('layer_group_id')
                    
                    # Map original task_ids to replayed task_ids
                    mapped_task_ids = []
                    for orig_id in original_task_ids:
                        orig_id = orig_id + (1+dp_client_id) * 10000000 #TODO remove this
                        #print(f"orig_id: {orig_id}")
                        if orig_id in request_id_mapping:
                            mapped_task_ids.append(request_id_mapping[orig_id])
                        else:
                            log_message(f"Warning: Cannot find mapping for task_id {orig_id}")
                            mapped_task_ids.append(orig_id)
                    
                    log_message(f"‚è∞‚è∞‚è∞Replaying {wait_type} for task_ids: {mapped_task_ids}")
                    
                    # ‰ΩøÁî®ÂêåÊ≠•Êé•Âè£
                    if wait_type in ["wait", "wait_for_graph_finished", "wait_at_layer_group"]:
                        # Map all blocking wait operations to basic wait
                        result = dp_client.wait(mapped_task_ids)
                    elif wait_type in ["try_wait", "try_wait_at_layer_group"]:
                        # Map all non-blocking wait operations to try_wait
                        result = dp_client.try_wait(mapped_task_ids)
                    else:
                        log_message(f"Unknown wait type: {wait_type}, using basic wait")
                        result = dp_client.wait(mapped_task_ids)
                    
                    successed_elements = []
                    for task_id in mapped_task_ids:
                        if task_id in result:
                            successed_elements.append(result[task_id].sum().item())
                        else:
                            successed_elements.append(0)
                    
                    print(f"wait result: task ids: {mapped_task_ids}, successed elements num: {successed_elements}")
                    
                    # Send response back to main process
                    response_queue.put({
                        'type': 'wait_completed',
                        'task_ids': mapped_task_ids,
                        'result': successed_elements
                    })
                
                # Small sleep between events to prevent blocking
                time.sleep(0.001)
                    
            except Exception as e:
                if not stop_event.is_set():
                    log_message(f"Error in request processing: {e}")
                    import traceback
                    traceback.print_exc()
                    
        log_message("DP client finished")
        
    except Exception as e:
        log_message(f"Error in DP client: {e}")
        import traceback
        traceback.print_exc()
        raise


class FlexKVServerClientReplayEngine:
    """Engine for replaying FlexKV trace files in server-client mode"""

    def __init__(self, trace_file: str, verbose: bool = True, gpu_blocks_num: int = 1000):
        self.trace_file = trace_file
        self.verbose = verbose
        self.events = []
        self.gpu_blocks_num = gpu_blocks_num
        self.server_recv_port = None
        self.server_process = None
        self.client_processes = []
        
        # Set global variables
        global global_verbose, global_gpu_blocks_num
        global_verbose = verbose
        global_gpu_blocks_num = gpu_blocks_num

    def log(self, message: str):
        """Log message if verbose mode is enabled"""
        if self.verbose:
            print(f"[REPLAY] {message}")

    def load_trace_file(self):
        """Load and parse the trace file"""
        self.log(f"Loading trace file: {self.trace_file}")

        try:
            with open(self.trace_file, encoding='utf-8') as f:
                lines = f.readlines()

            self.events = []
            for line_num, line in enumerate(lines, 1):
                try:
                    event = json.loads(line.strip())
                    self.events.append(event)
                except json.JSONDecodeError as e:
                    print(f"Warning: Failed to parse line {line_num}: {e}")
                    continue

            self.log(f"Loaded {len(self.events)} events from trace file")

        except FileNotFoundError:
            print(f"Error: Trace file '{self.trace_file}' not found")
            sys.exit(1)
        except Exception as e:
            print(f"Error loading trace file: {e}")
            sys.exit(1)

    def parse_config_event(self, event: Dict[str, Any]):
        """Parse configuration event and setup global configs"""
        self.log("Parsing configuration...")

        data = event['data']
        model_config_data = data['model_config']
        cache_config_data = data['cache_config']
        gpu_layout_data = data.get('gpu_layout')

        # Recreate model_config
        dtype_str = model_config_data['dtype']
        if dtype_str == "torch.float16":
            dtype = torch.float16
        elif dtype_str == "torch.float32":
            dtype = torch.float32
        elif dtype_str == "torch.bfloat16":
            dtype = torch.bfloat16
        else:
            dtype = torch.float16  # default

        # Set global model_config
        global global_model_config
        global_model_config = ModelConfig(
            num_layers=model_config_data['num_layers'],
            num_kv_heads=model_config_data['num_kv_heads'],
            head_size=8,#model_config_data['head_size'],
            use_mla=model_config_data['use_mla'],
            dtype=dtype,
            tp_size=1,#model_config_data['tp_size'],
            dp_size=1,#model_config_data['dp_size'],
        )

        # Set global cache_config (with trace disabled for replay)
        global global_cache_config
        global_cache_config = CacheConfig(
            tokens_per_block=cache_config_data['tokens_per_block'],
            enable_cpu=cache_config_data['enable_cpu'],
            enable_ssd=cache_config_data['enable_ssd'],
            enable_remote=cache_config_data['enable_remote'],
            gpu_kv_layout_type=self._parse_layout_type(cache_config_data['gpu_kv_layout_type']),
            cpu_kv_layout_type=self._parse_layout_type(cache_config_data['cpu_kv_layout_type']),
            ssd_kv_layout_type=self._parse_layout_type(cache_config_data['ssd_kv_layout_type']),
            remote_kv_layout_type=self._parse_layout_type(cache_config_data['remote_kv_layout_type']),
            use_gds=cache_config_data['use_gds'],
            use_pinned_memory=False,#cache_config_data['use_pinned_memory'],
            remote_cache_size_mode=cache_config_data['remote_cache_size_mode'],
            num_cpu_blocks=cache_config_data['num_cpu_blocks'],
            num_ssd_blocks=cache_config_data['num_ssd_blocks'],
            num_remote_blocks=cache_config_data['num_remote_blocks'],
            remote_file_size=cache_config_data['remote_file_size'],
            remote_file_num=cache_config_data['remote_file_num'],
            remote_file_prefix=cache_config_data['remote_file_prefix'],
            ssd_cache_dir=cache_config_data['ssd_cache_dir'],
            ssd_cache_iouring_entries=cache_config_data['ssd_cache_iouring_entries'],
            ssd_cache_iouring_flags=cache_config_data['ssd_cache_iouring_flags'],
            remote_cache_path=cache_config_data['remote_cache_path'],
            remote_config_custom=cache_config_data['remote_config_custom'],
            enable_trace=False,  # Disable trace for replay
        )

        # Set global gpu_layout if available
        global global_gpu_layout, global_gpu_blocks_num
        if gpu_layout_data:
            global_gpu_layout = KVCacheLayout(
                type=self._parse_layout_type(gpu_layout_data['type']),
                num_layer=gpu_layout_data['num_layer'],
                num_block=gpu_layout_data['num_block'],
                tokens_per_block=gpu_layout_data['tokens_per_block'],
                num_head=gpu_layout_data['num_head'],
                head_size=8,#gpu_layout_data['head_size'],
                is_mla=gpu_layout_data['is_mla'],
            )
        else:
            # Create default GPU layout if not provided in trace
            global_gpu_layout = KVCacheLayout(
                type=KVCacheLayoutType.LAYERWISE,
                num_layer=global_model_config.num_layers,
                num_block=global_gpu_blocks_num,
                tokens_per_block=global_cache_config.tokens_per_block,
                num_head=global_model_config.num_kv_heads // global_model_config.tp_size,
                head_size=8,#global_model_config.head_size,
                is_mla=global_model_config.use_mla
            )

        global_gpu_blocks_num = global_gpu_layout.num_block

        self.log(f"Model config: {global_model_config}")
        self.log(f"Cache config loaded")
        self.log(f"GPU layout: {global_gpu_layout}")

    def _parse_layout_type(self, layout_type_str: str) -> KVCacheLayoutType:
        """Parse layout type string to enum"""
        if "LAYERWISE" in layout_type_str:
            return KVCacheLayoutType.LAYERWISE
        elif "BLOCKWISE" in layout_type_str:
            return KVCacheLayoutType.BLOCKWISE
        else:
            return KVCacheLayoutType.LAYERWISE  # default

    

    def start_server_client_processes(self):
        """Start server and client processes"""
        # Create IPC socket path
        self.server_recv_port = f"ipc://{tempfile.NamedTemporaryFile(delete=False).name}"
        
        # Start server process (non-daemon, following test_multiprocess.py pattern)
        self.server_process = Process(
            target=run_server_process,
            args=(self.server_recv_port,),
            daemon=False  # Server should not be daemon
        )
        self.server_process.start()
        
        # Wait for server to start
        time.sleep(5)
        
        self.log("Server started successfully")

    def replay_all_events(self):
        """Replay all events in chronological order using server-client architecture"""
        self.log("Starting event replay in server-client mode...")

        config_events = [e for e in self.events if e['event_type'] == 'config']
        request_events = [e for e in self.events if e['event_type'] == 'request']
        wait_events = [e for e in self.events if e['event_type'] == 'wait']

        self.log(f"Found {len(config_events)} config, {len(request_events)} request, {len(wait_events)} wait events")

        # Parse configuration first
        if config_events:
            self.parse_config_event(config_events[0])
            self.log("Configuration parsed successfully")
        else:
            raise ValueError("No configuration found in trace file")

        # Start server and client processes
        self.start_server_client_processes()

        # Create queues for communication with client process
        request_queue = Queue()
        response_queue = Queue()
        stop_event = Event()

        # Start DP client process (non-daemon, following test_multiprocess.py pattern)
        dp_client_process = Process(
            target=run_dp_client_process,
            args=(0, self.server_recv_port, request_queue, response_queue, stop_event),
            daemon=False  # DP client should not be daemon
        )
        dp_client_process.start()
        self.client_processes.append(dp_client_process)

        # Wait for client to be ready
        time.sleep(13)

        # Sort and replay events
        other_events = request_events + wait_events
        other_events.sort(key=lambda e: e['timestamp'])

        replayed_event_num = 0
        for event in other_events:
            # Send event to client process
            request_queue.put({
                'event_type': event['event_type'],
                'data': event['data']
            })
            
            # Wait for response
            try:
                response = response_queue.get(timeout=30)  # 30 second timeout
                if response['type'] == 'request_completed':
                    self.log(f"Request completed: {response['original_request_id']} -> {response['task_id']}")
                elif response['type'] == 'wait_completed':
                    self.log(f"Wait completed for task_ids: {response['task_ids']}")
                elif response['type'] == 'wait_failed':
                    self.log(f"Wait failed for task_ids: {response['task_ids']}, error: {response['error']}")
                
            except Exception as e:
                self.log(f"Error waiting for response: {e}")
                break

            # Small delay between events
            time.sleep(0.001)
            replayed_event_num += 1

        # Signal client to stop
        request_queue.put(None)
        stop_event.set()

        self.log(f"Event replay completed successfully! Replayed {replayed_event_num} events")

    def cleanup(self):
        """Cleanup resources"""
        self.log("Starting cleanup...")
        
        # Terminate client processes
        for process in self.client_processes:
            if process.is_alive():
                process.terminate()
                process.join(timeout=5)
                if process.is_alive():
                    process.kill()
        
        # Terminate server process
        if self.server_process and self.server_process.is_alive():
            self.server_process.terminate()
            self.server_process.join(timeout=5)
            if self.server_process.is_alive():
                self.server_process.kill()
        
        self.log("Cleanup completed")


def main():
    parser = argparse.ArgumentParser(description="Replay FlexKV trace files in server-client mode")
    parser.add_argument("--trace-file", required=True, help="Path to the trace file")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
    parser.add_argument("--gpu-blocks", type=int, default=1000, help="Number of GPU blocks to create")

    args = parser.parse_args()

    replay_engine = None
    
    def signal_handler(signum, frame):
        """Handle SIGINT (Ctrl+C) to gracefully shut down"""
        print("\n‚ùå Received interrupt signal, shutting down...")
        if replay_engine:
            replay_engine.cleanup()
        sys.exit(0)
    
    # Register signal handler
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        replay_engine = FlexKVServerClientReplayEngine(args.trace_file, args.verbose, args.gpu_blocks)
        replay_engine.load_trace_file()
        replay_engine.replay_all_events()

        print("‚úÖ Server-client replay completed successfully!")

    except KeyboardInterrupt:
        print("\n‚ùå Replay interrupted by user")
    except Exception as e:
        print(f"‚ùå Replay failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        if replay_engine:
            replay_engine.cleanup()


if __name__ == "__main__":
    main() 