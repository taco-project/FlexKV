#!/usr/bin/env python3
"""
Replay script for FlexKV trace files

This script can read trace files generated by FlexKV tracer and replay the entire
workflow including:
1. Configuration setup (model_config, cache_config, gpu_layout)
2. GPU blocks creation
3. KVManager initialization
4. Request replay (get_async, put_async)
5. Wait operations replay

Usage:
    python replay_from_tracer.py --trace-file ./flexkv_trace.log --verbose
"""

import argparse
import json
import sys
import time
from typing import Dict, List, Optional, Any, Tuple
import torch
import zmq

from flexkv.common.config import CacheConfig, ModelConfig
from flexkv.common.storage import KVCacheLayout, KVCacheLayoutType
from flexkv.common.memory_handle import TensorSharedHandle
from flexkv.kvtask import KVTaskEngine
from flexkv.server.request import RegisterTPClientRequest
from flexkv.server.utils import get_zmq_socket


class FlexKVReplayEngine:
    """Engine for replaying FlexKV trace files"""

    def __init__(self, trace_file: str, verbose: bool = True, gpu_blocks_num: int = 1000):
        self.trace_file = trace_file
        self.verbose = verbose
        self.events = []
        self.model_config = None
        self.cache_config = None
        self.gpu_layout = None
        self.kvmanager = None
        self.gpu_blocks_num = gpu_blocks_num
        self.gpu_blocks = {}

    def log(self, message: str):
        """Log message if verbose mode is enabled"""
        if self.verbose:
            print(f"[REPLAY] {message}")

    def load_trace_file(self):
        """Load and parse the trace file"""
        self.log(f"Loading trace file: {self.trace_file}")

        try:
            with open(self.trace_file, encoding='utf-8') as f:
                lines = f.readlines()

            self.events = []
            for line_num, line in enumerate(lines, 1):
                try:
                    event = json.loads(line.strip())
                    self.events.append(event)
                except json.JSONDecodeError as e:
                    print(f"Warning: Failed to parse line {line_num}: {e}")
                    continue

            self.log(f"Loaded {len(self.events)} events from trace file")

        except FileNotFoundError:
            print(f"Error: Trace file '{self.trace_file}' not found")
            sys.exit(1)
        except Exception as e:
            print(f"Error loading trace file: {e}")
            sys.exit(1)

    def parse_config_event(self, event: Dict[str, Any]):
        """Parse configuration event and setup configs"""
        self.log("Parsing configuration...")

        data = event['data']
        model_config_data = data['model_config']
        cache_config_data = data['cache_config']
        global_config_data = data.get('global_config', {})
        gpu_layout_data = data.get('gpu_layout')

        # Restore GLOBAL_CONFIG_FROM_ENV from trace
        if global_config_data:
            self.log("Restoring global config from trace...")
            from flexkv.common.config import GLOBAL_CONFIG_FROM_ENV
            
            # Restore layout types
            if 'cpu_layout_type' in global_config_data:
                GLOBAL_CONFIG_FROM_ENV.cpu_layout_type = self._parse_layout_type(global_config_data['cpu_layout_type'])
            if 'ssd_layout_type' in global_config_data:
                GLOBAL_CONFIG_FROM_ENV.ssd_layout_type = self._parse_layout_type(global_config_data['ssd_layout_type'])
            if 'remote_layout_type' in global_config_data:
                GLOBAL_CONFIG_FROM_ENV.remote_layout_type = self._parse_layout_type(global_config_data['remote_layout_type'])
            if 'gds_layout_type' in global_config_data:
                GLOBAL_CONFIG_FROM_ENV.gds_layout_type = self._parse_layout_type(global_config_data['gds_layout_type'])
            
            # Restore other configs
            for key in ['server_client_mode', 'index_accel', 'use_ce_transfer_h2d', 'use_ce_transfer_d2h',
                       'transfer_sms_h2d', 'transfer_sms_d2h', 'iouring_entries', 'iouring_flags',
                       'max_file_size_gb', 'evict_ratio', 'server_recv_port']:
                if key in global_config_data:
                    setattr(GLOBAL_CONFIG_FROM_ENV, key, global_config_data[key])
                    self.log(f"  Restored {key} = {global_config_data[key]}")

        # Recreate model_config
        dtype_str = model_config_data['dtype']
        if dtype_str == "torch.float16":
            dtype = torch.float16
        elif dtype_str == "torch.float32":
            dtype = torch.float32
        elif dtype_str == "torch.bfloat16":
            dtype = torch.bfloat16
        else:
            dtype = torch.float16  # default

        self.model_config = ModelConfig(
            num_layers=model_config_data['num_layers'],
            num_kv_heads=model_config_data['num_kv_heads'],
            head_size=8,#model_config_data['head_size'], # for local test
            use_mla=model_config_data['use_mla'],
            dtype=dtype,
            tp_size=1,#model_config_data['tp_size'], # for local test
            dp_size=1,#model_config_data['dp_size'], # for local test
        )

        # Recreate cache_config (with trace disabled for replay)
        self.cache_config = CacheConfig(
            tokens_per_block=cache_config_data['tokens_per_block'],
            enable_cpu=cache_config_data['enable_cpu'],
            enable_ssd=cache_config_data['enable_ssd'],
            enable_remote=cache_config_data['enable_remote'],
            enable_gds=cache_config_data['enable_gds'],
            num_cpu_blocks=cache_config_data['num_cpu_blocks'],
            num_ssd_blocks=cache_config_data['num_ssd_blocks'],
            num_gds_blocks=cache_config_data['num_gds_blocks'],
            num_remote_blocks=cache_config_data['num_remote_blocks'],
            ssd_cache_dir=cache_config_data['ssd_cache_dir'],
            gds_cache_dir=cache_config_data['gds_cache_dir'],
            remote_cache_size_mode=cache_config_data['remote_cache_size_mode'],
            remote_file_size=cache_config_data['remote_file_size'],
            remote_file_num=cache_config_data['remote_file_num'],
            remote_file_prefix=cache_config_data['remote_file_prefix'],
            remote_cache_path=cache_config_data['remote_cache_path'],
            remote_config_custom=cache_config_data['remote_config_custom'],
        )

        # Recreate gpu_layout if available
        if gpu_layout_data:
            self.gpu_layout = KVCacheLayout(
                type=self._parse_layout_type(gpu_layout_data['type']),
                num_layer=gpu_layout_data['num_layer'],
                num_block=gpu_layout_data['num_block'],
                tokens_per_block=gpu_layout_data['tokens_per_block'],
                num_head=gpu_layout_data['num_head'],
                head_size=8,#gpu_layout_data['head_size'], #for local test
                is_mla=gpu_layout_data['is_mla'],
            )
            self.gpu_blocks_num = self.gpu_layout.num_block

        self.log(f"Model config: {self.model_config}")
        self.log(f"Cache config loaded {self.cache_config}")
        if self.gpu_layout:
            self.log(f"GPU layout: {self.gpu_layout}")

    def _parse_layout_type(self, layout_type_str: str) -> KVCacheLayoutType:
        """Parse layout type string to enum"""
        if "LAYERFIRST" in layout_type_str:
            return KVCacheLayoutType.LAYERFIRST
        elif "BLOCKFIRST" in layout_type_str:
            return KVCacheLayoutType.BLOCKFIRST
        else:
            return KVCacheLayoutType.LAYERFIRST  # default

    def create_gpu_blocks(self):
        """Create GPU blocks for testing (similar to test code)"""
        self.log("Creating GPU blocks...")

        total_gpus = self.model_config.tp_size * self.model_config.dp_size
        available_gpus = torch.cuda.device_count()

        if available_gpus < total_gpus:
            self.log(f"Warning: Need {total_gpus} GPUs but only {available_gpus} available. Using GPU 0 for all.")

        self.gpu_blocks = {}
        for gpu_id in range(total_gpus):
            self.gpu_blocks[gpu_id] = []
            # Use available GPU or fallback to GPU 0
            device_name = f"cuda:{gpu_id}" if gpu_id < available_gpus else "cuda:0"

            for layer_id in range(self.model_config.num_layers):
                # Create KV cache tensor: [2, num_blocks, tokens_per_block, num_heads, head_size]
                kv_dim = 2 if not self.model_config.use_mla else 1
                kv_tensor = torch.zeros(  # Use zeros instead of random for reproducibility
                    size=(kv_dim, self.gpu_blocks_num, self.cache_config.tokens_per_block,
                          self.model_config.num_kv_heads // self.model_config.tp_size,
                          self.model_config.head_size),
                    dtype=self.model_config.dtype,
                    device=device_name
                )
                self.gpu_blocks[gpu_id].append(kv_tensor)

        self.log(f"Created GPU blocks for {total_gpus} GPUs with {self.gpu_blocks_num} blocks each")

    def register_gpu_blocks_to_kvmanager(self, gpu_register_port: str):
        """Register GPU blocks to KVManager via socket"""
        self.log("Registering GPU blocks via socket...")
        
        total_gpus = self.model_config.tp_size * self.model_config.dp_size
        
        # Create zmq socket to send GPU blocks
        context = zmq.Context(2)
        send_socket = get_zmq_socket(
            context, zmq.SocketType.PUSH, gpu_register_port, False
        )
        
        # Register each GPU's blocks
        for gpu_id in range(total_gpus):
            # Convert torch tensors to TensorSharedHandle
            handles = []
            for layer_tensor in self.gpu_blocks[gpu_id]:
                handle = TensorSharedHandle(layer_tensor, gpu_id)
                handles.append(handle)
            
            # Create registration request
            register_req = RegisterTPClientRequest(
                dp_client_id=gpu_id // self.model_config.tp_size,  # DP client ID
                device_id=gpu_id,
                handles=handles,
                gpu_layout=self.gpu_layout
            )
            
            # Send registration request
            send_socket.send_pyobj(register_req)
            self.log(f"Registered GPU {gpu_id} blocks")
        
        # Wait a bit to ensure all registration requests are sent
        time.sleep(0.1)
        
        # Close socket
        send_socket.close()
        context.term()
        self.log("GPU blocks registration completed")

    def create_kvmanager(self,):
        """Create and initialize KVManager"""
        self.log("Creating KVManager...")

        if not self.gpu_layout:
            # Create default GPU layout if not provided in trace
            self.gpu_layout = KVCacheLayout(
                type=KVCacheLayoutType.LAYERFIRST,
                num_layer=self.model_config.num_layers,
                num_block=self.gpu_blocks_num,  # default number of blocks
                tokens_per_block=self.cache_config.tokens_per_block,
                num_head=self.model_config.num_kv_heads // self.model_config.tp_size,
                head_size=self.model_config.head_size,
                is_mla=self.model_config.use_mla
            )

        # Create KVTaskEngine with gpu_register_port
        import tempfile
        gpu_register_port = f"ipc://{tempfile.NamedTemporaryFile(delete=False).name}"
        
        self.kvmanager = KVTaskEngine(
            model_config=self.model_config,
            cache_config=self.cache_config,
            gpu_register_port=gpu_register_port
        )

        # Start KVManager first so it can listen for registration requests
        self.kvmanager.start()

        # Register GPU blocks via socket after KVManager is started
        self.register_gpu_blocks_to_kvmanager(gpu_register_port)
        
        # Wait for KVManager to be ready
        max_wait_time = 30  # seconds
        start_time = time.time()
        while not self.kvmanager.is_ready():
            if time.time() - start_time > max_wait_time:
                raise RuntimeError("KVManager failed to become ready within timeout")
            time.sleep(0.1)
        
        self.log("KVManager started successfully")

    def replay_request_event(self, event: Dict[str, Any]) -> int:
        """Replay a request event (GET, PUT, GET_MATCH, PUT_MATCH)"""
        data = event['data']
        request_type = data['request_type']

        # Convert lists back to numpy arrays (KVTaskEngine uses numpy, not torch)
        import numpy as np
        token_ids = np.array(data['token_ids'], dtype=np.int64)
        slot_mapping = np.array(data['slot_mapping'], dtype=np.int64)
        token_mask = np.array(data['token_mask'], dtype=bool) if data['token_mask'] else None
        layer_granularity = data.get('layer_granularity', -1)
        dp_id = data.get('dp_id', 0)

        self.log(f"Replaying {request_type} request with {len(token_ids)} tokens")

        if request_type == "GET":
            print(f"üîçüîçüîçGET token_ids: {token_ids[:128]}")
            print(f"request_id: {data['request_id']}, request_type: {request_type}, "
                  f"input length: {len(token_ids)}, true in mask: {token_mask.sum() if token_mask is not None else 'N/A'}")
            # get_async return (task_id, return_mask)
            task_id, return_mask = self.kvmanager.get_async(
                token_ids=token_ids,
                slot_mapping=slot_mapping,
                token_mask=token_mask,
                layer_granularity=layer_granularity,
                dp_id=dp_id
            )
        elif request_type == "PUT":
            print(f"‚úÖ‚úÖ‚úÖPUT token_ids: {token_ids[:128]}")
            print(f"request_id: {data['request_id']}, request_type: {request_type}, "
                  f"input length: {len(token_ids)}, true in mask: {token_mask.sum() if token_mask is not None else 'N/A'}")
            # put_async return (task_id, return_mask)
            task_id, return_mask = self.kvmanager.put_async(
                token_ids=token_ids,
                slot_mapping=slot_mapping,
                token_mask=token_mask,
                dp_id=dp_id
            )
        elif request_type == "GET_MATCH":
            print(f"üîçüìùGET_MATCH token_ids: {token_ids[:128]}")
            print(f"request_id: {data['request_id']}, request_type: {request_type}, "
                  f"input length: {len(token_ids)}, true in mask: {token_mask.sum() if token_mask is not None else 'N/A'}")
            # get_match return (task_id, return_mask)
            task_id, return_mask = self.kvmanager.get_match(
                token_ids=token_ids,
                token_mask=token_mask,
                layer_granularity=layer_granularity,
                dp_id=dp_id
            )
        elif request_type == "PUT_MATCH":
            print(f"‚úÖüìùPUT_MATCH token_ids: {token_ids[:128]}")
            print(f"request_id: {data['request_id']}, request_type: {request_type}, "
                  f"input length: {len(token_ids)}, true in mask: {token_mask.sum() if token_mask is not None else 'N/A'}")
            # put_match return (task_id, return_mask)
            task_id, return_mask = self.kvmanager.put_match(
                token_ids=token_ids,
                token_mask=token_mask,
                dp_id=dp_id
            )
        else:
            raise ValueError(f"Unknown request type: {request_type}")

        return task_id

    def replay_launch_tasks_event(self, event: Dict[str, Any]):
        """Replay a launch_tasks event"""
        data = event['data']
        task_ids = data['task_ids']
        slot_mappings_list = data['slot_mappings']
        as_batch = data.get('as_batch', False)
        batch_id = data.get('batch_id', -1)
        self.log(f"üöÄüöÄüöÄReplaying launch_tasks for task_ids: {task_ids}")
        
        try:
            # Convert lists back to numpy arrays
            import numpy as np
            slot_mappings = [np.array(sm, dtype=np.int64) for sm in slot_mappings_list]
            
            print(f"Launching {len(task_ids)} tasks with slot_mappings")
            
            # Call launch_tasks
            self.kvmanager.launch_tasks(task_ids, slot_mappings, as_batch, batch_id)
            
            self.log(f"launch_tasks completed successfully for {len(task_ids)} tasks")
            
        except Exception as e:
            self.log(f"Warning: launch_tasks operation failed: {e}")
            import traceback
            traceback.print_exc()

    def replay_wait_event(self, event: Dict[str, Any]):
        """Replay a wait event"""
        data = event['data']
        wait_type = data['wait_type']
        task_ids = data['task_ids']
        timeout = data.get('timeout', 20.0)  # default timeout
        completely = data.get('completely', False)  # default completely
        layer_group_id = data.get('layer_group_id')

        self.log(f"‚è∞‚è∞‚è∞Replaying {wait_type} for task_ids: {task_ids}, timeout: {timeout}, completely: {completely}")

        try:
            # wait and try_wait return Dict[int, KVResponse]
            if wait_type == "wait":
                result = self.kvmanager.wait(task_ids, timeout=timeout, completely=completely)
            elif wait_type == "try_wait":
                result = self.kvmanager.try_wait(task_ids)
            else:
                raise ValueError(f"Unknown wait type: {wait_type}")
            
            # process result: result is Dict[int, KVResponse]
            successed_elements = []
            statuses = []
            for task_id in task_ids:
                if task_id in result:
                    # return_mask in KVResponse may be None
                    if result[task_id].return_mask is not None:
                        successed_elements.append(result[task_id].return_mask.sum())
                    else:
                        successed_elements.append(0)
                    statuses.append(result[task_id].status.name if hasattr(result[task_id], 'status') else "SUCCESS")
                else:
                    successed_elements.append(0)
                    statuses.append("NOT_FOUND")
            
            print(f"‚úÖ {wait_type} result: task_ids={task_ids}, successed_elements={successed_elements}, statuses={statuses}")
            self.log(f"Wait completed successfully for {wait_type}")
            return result

        except Exception as e:
            self.log(f"Warning: Wait operation failed: {e}")
            import traceback
            traceback.print_exc()
            return None

    def replay_all_events(self):
        """Replay all events in chronological order"""
        self.log("Starting event replay...")

        config_events = [e for e in self.events if e['event_type'] == 'config']
        request_events = [e for e in self.events if e['event_type'] == 'request']
        wait_events = [e for e in self.events if e['event_type'] == 'wait']
        launch_tasks_events = [e for e in self.events if e['event_type'] == 'launch_tasks']

        self.log(f"Found {len(config_events)} config, {len(request_events)} request, "
                f"{len(wait_events)} wait, {len(launch_tasks_events)} launch_tasks events")

        # Parse configuration first
        if config_events:
            self.parse_config_event(config_events[0])
            print("parse_config_event done")
        else:
            raise ValueError("No configuration found in trace file")

        # Create GPU blocks and KVManager
        self.create_gpu_blocks()
        self.create_kvmanager()
        # Replay all non-config events in timestamp order
        other_events = request_events + wait_events + launch_tasks_events
        other_events.sort(key=lambda e: e['timestamp'])

        request_id_mapping = {}  # Map original request_id to replayed task_id
        for replayed_event_num, event in enumerate(other_events):
            event_type = event['event_type']

            if event_type == 'request':
                original_request_id = event['data']['request_id']
                replayed_task_id = self.replay_request_event(event)
                request_id_mapping[original_request_id] = replayed_task_id
                self.log(f"Mapped original request_id {original_request_id} to task_id {replayed_task_id}")

            elif event_type == 'launch_tasks':
                # Map original task_ids to replayed task_ids
                original_task_ids = event['data']['task_ids']
                mapped_task_ids = []
                for orig_id in original_task_ids:
                    if orig_id in request_id_mapping:
                        mapped_task_ids.append(request_id_mapping[orig_id])
                    else:
                        self.log(f"Warning: Cannot find mapping for task_id {orig_id}")
                        mapped_task_ids.append(orig_id)  # Use original if not found

                # Update event data with mapped task_ids
                event['data']['task_ids'] = mapped_task_ids
                self.replay_launch_tasks_event(event)
                print("launch_tasks done")

            elif event_type == 'wait':
                # Map original task_ids to replayed task_ids
                original_task_ids = event['data']['task_ids']
                mapped_task_ids = []
                for orig_id in original_task_ids:
                    if orig_id in request_id_mapping:
                        mapped_task_ids.append(request_id_mapping[orig_id])
                    else:
                        self.log(f"Warning: Cannot find mapping for task_id {orig_id}")
                        mapped_task_ids.append(orig_id)  # Use original if not found

                # Update event data with mapped task_ids
                event['data']['task_ids'] = mapped_task_ids
                results = self.replay_wait_event(event)
                print("wait request done")

            # Small delay between events to simulate real timing
            time.sleep(0.001)
            #if replayed_event_num == 200:
            #    break

        self.log("Event replay completed successfully!")

    def cleanup(self):
        """Cleanup resources"""
        if self.kvmanager:
            self.kvmanager.shutdown()
            self.log("KVManager shutdown completed")


def main():
    parser = argparse.ArgumentParser(description="Replay FlexKV trace files")
    parser.add_argument("--trace-file", required=True, help="Path to the trace file")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
    parser.add_argument("--gpu-blocks", type=int, default=1000, help="Number of GPU blocks to create")

    args = parser.parse_args()

    try:
        replay_engine = FlexKVReplayEngine(args.trace_file, args.verbose, args.gpu_blocks)
        replay_engine.load_trace_file()
        replay_engine.replay_all_events()

        print("‚úÖ Replay completed successfully!")

    except KeyboardInterrupt:
        print("\n‚ùå Replay interrupted by user")
    except Exception as e:
        print(f"‚ùå Replay failed: {e}")
        sys.exit(1)
    finally:
        if 'replay_engine' in locals():
            replay_engine.cleanup()


if __name__ == "__main__":
    main()
