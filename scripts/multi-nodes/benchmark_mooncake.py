#!/usr/bin/env python3
"""
Mooncake Trace Benchmark è„šæœ¬
æ­£ç¡®å¤„ç† hash_id åˆ° token çš„æ˜ å°„ï¼Œæ”¯æŒ prefix cache æµ‹è¯•

Mooncake trace æ ¼å¼:
{
    "timestamp": 27482,        # æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
    "input_length": 6955,      # è¾“å…¥ token æ•°
    "output_length": 52,       # è¾“å‡º token æ•°
    "hash_ids": [46, 47, ...]  # hash ID åˆ—è¡¨ï¼ˆæ¯ä¸ªä»£è¡¨ä¸€ä¸ª KV cache blockï¼‰
}

è®¾è®¡æ€è·¯ï¼š
- æ¯ä¸ª hash_id å¯¹åº”ä¸€ä¸ªå›ºå®šçš„ token blockï¼ˆé»˜è®¤ 512 tokensï¼‰
- ç›¸åŒ hash_id ç”Ÿæˆç›¸åŒçš„ token åºåˆ—ï¼Œç¡®ä¿ prefix cache å¯å¤ç”¨
- ä½¿ç”¨ prompt_token_ids ç›´æ¥å‘é€ token IDsï¼ˆå¦‚æœæ”¯æŒï¼‰ï¼Œå¦åˆ™ç”¨æ–‡æœ¬
"""

import json
import time
import asyncio
import aiohttp
import argparse
import random
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from collections import defaultdict
from transformers import AutoTokenizer


@dataclass
class MooncakeRequest:
    """Mooncake è¯·æ±‚æ•°æ®ç»“æ„"""
    timestamp: int  # æ¯«ç§’
    input_length: int
    output_length: int
    hash_ids: List[int]
    request_id: int = 0


class TokenBlockGenerator:
    """
    Token Block ç”Ÿæˆå™¨
    æ¯ä¸ª hash_id æ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šçš„ token block
    """
    
    def __init__(self, tokenizer_path: str, block_size: int = 512, seed: int = 42):
        """
        Args:
            tokenizer_path: tokenizer è·¯å¾„
            block_size: æ¯ä¸ª hash block çš„ token æ•°é‡
            seed: éšæœºç§å­
        """
        print(f"åŠ è½½ tokenizer: {tokenizer_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
        self.block_size = block_size
        self.seed = seed
        
        # ç¼“å­˜ï¼šhash_id -> token_ids
        self.hash_to_tokens: Dict[int, List[int]] = {}
        
        # è·å–è¯æ±‡è¡¨å¤§å°ï¼Œç”¨äºç”Ÿæˆéšæœº token
        self.vocab_size = self.tokenizer.vocab_size
        
        # æ’é™¤ç‰¹æ®Š token
        self.special_token_ids = set(self.tokenizer.all_special_ids)
        
        print(f"Tokenizer åŠ è½½å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {self.vocab_size}, Block å¤§å°: {block_size}")
    
    def _generate_block_tokens(self, hash_id: int) -> List[int]:
        """ä¸ºç‰¹å®š hash_id ç”Ÿæˆå›ºå®šçš„ token block"""
        if hash_id in self.hash_to_tokens:
            return self.hash_to_tokens[hash_id]
        
        # ä½¿ç”¨ hash_id ä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒ hash_id ç”Ÿæˆç›¸åŒçš„ tokens
        rng = random.Random(self.seed + hash_id * 12345)
        
        tokens = []
        while len(tokens) < self.block_size:
            # ç”Ÿæˆéšæœº token IDï¼Œé¿å¼€ç‰¹æ®Š token
            token_id = rng.randint(100, self.vocab_size - 1)
            if token_id not in self.special_token_ids:
                tokens.append(token_id)
        
        tokens = tokens[:self.block_size]
        self.hash_to_tokens[hash_id] = tokens
        return tokens
    
    def generate_prompt_tokens(self, hash_ids: List[int], target_length: int) -> List[int]:
        """
        æ ¹æ® hash_ids å’Œç›®æ ‡é•¿åº¦ç”Ÿæˆ prompt token IDs
        
        Args:
            hash_ids: hash ID åˆ—è¡¨
            target_length: ç›®æ ‡ token æ•°
            
        Returns:
            token ID åˆ—è¡¨
        """
        # æ‹¼æ¥æ‰€æœ‰ hash block çš„ tokens
        all_tokens = []
        for hash_id in hash_ids:
            block_tokens = self._generate_block_tokens(hash_id)
            all_tokens.extend(block_tokens)
        
        # è°ƒæ•´åˆ°ç›®æ ‡é•¿åº¦
        if len(all_tokens) >= target_length:
            # æˆªæ–­
            return all_tokens[:target_length]
        else:
            # å¡«å……ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ª hash_id çš„å»¶ç»­æˆ–éšæœº tokensï¼‰
            rng = random.Random(self.seed + target_length)
            while len(all_tokens) < target_length:
                token_id = rng.randint(100, self.vocab_size - 1)
                if token_id not in self.special_token_ids:
                    all_tokens.append(token_id)
            return all_tokens[:target_length]
    
    def tokens_to_text(self, token_ids: List[int]) -> str:
        """å°† token IDs è½¬æ¢ä¸ºæ–‡æœ¬"""
        return self.tokenizer.decode(token_ids, skip_special_tokens=True)


class MooncakeBenchmark:
    """Mooncake Trace Benchmark ä¸»ç±»"""
    
    def __init__(
        self,
        hosts: List[tuple],
        tokenizer_path: str,
        block_size: int = 512,
        model_name: str = None,
        timeout: int = 300,
        use_token_ids: bool = False,  # æ˜¯å¦ä½¿ç”¨ prompt_token_ids
    ):
        self.hosts = hosts
        self.base_urls = [f"http://{host}:{port}/v1" for host, port in hosts]
        self.model_name = model_name or tokenizer_path
        self.timeout = timeout
        self.use_token_ids = use_token_ids
        self.host_counter = 0
        
        # Token ç”Ÿæˆå™¨
        self.token_generator = TokenBlockGenerator(tokenizer_path, block_size)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.results: List[Dict[str, Any]] = []
    
    def get_next_url(self, strategy: str = 'round-robin') -> str:
        """è·å–ä¸‹ä¸€ä¸ªç›®æ ‡ URL"""
        if strategy == 'round-robin':
            url = self.base_urls[self.host_counter % len(self.base_urls)]
            self.host_counter += 1
            return url
        elif strategy == 'random':
            return random.choice(self.base_urls)
        else:
            return self.base_urls[0]
    
    def get_url_by_hash(self, hash_ids: List[int]) -> str:
        """æ ¹æ® hash_ids é€‰æ‹©èŠ‚ç‚¹"""
        if not hash_ids:
            return random.choice(self.base_urls)
        return self.base_urls[hash_ids[0] % len(self.base_urls)]
    
    async def send_request(
        self,
        session: aiohttp.ClientSession,
        request: MooncakeRequest,
        base_url: str,
        semaphore: asyncio.Semaphore,
    ) -> Dict[str, Any]:
        """å‘é€å•ä¸ªè¯·æ±‚"""
        async with semaphore:
            
            # ç”Ÿæˆ prompt tokens
            prompt_tokens = self.token_generator.generate_prompt_tokens(
                request.hash_ids,
                request.input_length
            )
            
            # æ„å»ºè¯·æ±‚
            if self.use_token_ids:
                # ä½¿ç”¨ prompt_token_idsï¼ˆéœ€è¦ vLLM æ”¯æŒï¼‰
                payload = {
                    "model": self.model_name,
                    "prompt_token_ids": prompt_tokens,
                    "max_tokens": request.output_length,
                    "temperature": 0.7,
                    "stream": True,
                    "stream_options": {"include_usage": True},
                }
            else:
                # è½¬æ¢ä¸ºæ–‡æœ¬
                prompt_text = self.token_generator.tokens_to_text(prompt_tokens)
                payload = {
                    "model": self.model_name,
                    "prompt": prompt_text,
                    "max_tokens": request.output_length,
                    "temperature": 0.7,
                    "stream": True,
                    "stream_options": {"include_usage": True},
                }
            
            start_time = time.time()
            first_token_time = None
            completion_tokens = 0
            prompt_tokens_actual = 0
            
            try:
                url = f"{base_url}/completions"
                async with session.post(
                    url,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                ) as response:
                    if response is None:
                        return {
                            'success': False,
                            'request_id': request.request_id,
                            'base_url': base_url,
                            'error': 'Response is None',
                            'timestamp': request.timestamp,
                        }
                    
                    if response.status != 200:
                        error_text = await response.text()
                        return {
                            'success': False,
                            'request_id': request.request_id,
                            'base_url': base_url,
                            'error': f"HTTP {response.status}: {error_text[:200]}",
                            'timestamp': request.timestamp,
                        }
                    
                    # æµå¼è¯»å–å“åº”
                    async for line in response.content:
                        line = line.decode('utf-8').strip()
                        if not line or not line.startswith('data: '):
                            continue
                        
                        data_str = line[6:]
                        if data_str == '[DONE]':
                            break
                        
                        try:
                            data = json.loads(data_str)
                            
                            # è®°å½•é¦– token æ—¶é—´
                            choices = data.get('choices', [])
                            if first_token_time is None and choices:
                                # choices å¯èƒ½æ˜¯ç©ºåˆ—è¡¨ï¼ˆusage chunkï¼‰
                                if len(choices) > 0 and choices[0].get('text'):
                                    first_token_time = time.time()
                            
                            # æå– usage ä¿¡æ¯
                            usage = data.get('usage')
                            if usage:
                                prompt_tokens_actual = usage.get('prompt_tokens', 0)
                                completion_tokens = usage.get('completion_tokens', 0)
                        except json.JSONDecodeError:
                            continue
                    
                    end_time = time.time()
                    
                    # è®¡ç®—æŒ‡æ ‡
                    ttft = first_token_time - start_time if first_token_time else 0
                    total_time = end_time - start_time
                    decode_time = end_time - first_token_time if first_token_time else 0
                    
                    result = {
                        'success': True,
                        'request_id': request.request_id,
                        'base_url': base_url,
                        'timestamp': request.timestamp,
                        'target_input_length': request.input_length,
                        'target_output_length': request.output_length,
                        'actual_prompt_tokens': prompt_tokens_actual,
                        'actual_completion_tokens': completion_tokens,
                        'hash_ids': request.hash_ids,
                        'hash_prefix_len': len(request.hash_ids),
                        'ttft': ttft,
                        'total_time': total_time,
                        'decode_time': decode_time,
                        'prefill_speed': prompt_tokens_actual / ttft if ttft > 0 else 0,
                        'decode_speed': (completion_tokens - 1) / decode_time if decode_time > 0 and completion_tokens > 1 else 0,
                        'start_time': start_time,
                        'end_time': end_time,
                    }
                    
                    port = base_url.split(':')[-1].split('/')[0]
                    print(f"[Req {request.request_id:4d}] Port: {port}, "
                          f"Prompt: {prompt_tokens_actual:5d} tokens (target: {request.input_length}), "
                          f"TTFT: {ttft:.3f}s, "
                          f"Prefill: {result['prefill_speed']:7.1f} tok/s, "
                          f"Output: {completion_tokens:3d} tokens, "
                          f"Decode: {result['decode_speed']:6.1f} tok/s, "
                          f"HashBlocks: {len(request.hash_ids)}")
                    
                    return result
                    
            except asyncio.TimeoutError:
                return {
                    'success': False,
                    'request_id': request.request_id,
                    'base_url': base_url,
                    'error': 'Timeout',
                    'timestamp': request.timestamp,
                }
            except Exception as e:
                return {
                    'success': False,
                    'request_id': request.request_id,
                    'base_url': base_url,
                    'error': str(e),
                    'timestamp': request.timestamp,
                }
    
    async def run_benchmark(
        self,
        requests: List[MooncakeRequest],
        strategy: str = 'round-robin',
        max_concurrency: int = 64,
        time_scale: float = 1.0,
        use_timestamp: bool = True,
    ):
        """è¿è¡Œ benchmark"""
        print("="*80)
        print(" Mooncake Trace Benchmark")
        print("="*80)
        print(f"è¯·æ±‚æ•°é‡: {len(requests)}")
        print(f"è´Ÿè½½å‡è¡¡ç­–ç•¥: {strategy}")
        print(f"æœ€å¤§å¹¶å‘æ•°: {max_concurrency}")
        print(f"æŒ‰æ—¶é—´æˆ³å‘é€: {use_timestamp}")
        print(f"Block å¤§å°: {self.token_generator.block_size} tokens")
        if use_timestamp:
            print(f"æ—¶é—´ç¼©æ”¾å› å­: {time_scale}x")
        print(f"ç›®æ ‡èŠ‚ç‚¹:")
        for i, (host, port) in enumerate(self.hosts, 1):
            print(f"  èŠ‚ç‚¹ {i}: {host}:{port}")
        print("="*80)
        print()
        
        # å¤„ç†è¯·æ±‚é¡ºåº
        if strategy == 'shuffle':
            # Shuffle ç­–ç•¥ï¼šæ‰“ä¹±è¯·æ±‚é¡ºåºï¼Œç„¶åç”¨ round-robin åˆ†å‘
            # è¿™æ ·ç›¸åŒå‰ç¼€çš„è¯·æ±‚æ›´å¯èƒ½è¢«åˆ†é…åˆ°ä¸åŒèŠ‚ç‚¹ï¼Œæµ‹è¯•è·¨èŠ‚ç‚¹ KV cache reuse
            print("ğŸ“Š Shuffle ç­–ç•¥ï¼šæ‰“ä¹±è¯·æ±‚é¡ºåºåä½¿ç”¨ round-robin åˆ†å‘")
            sorted_requests = list(requests)
            random.shuffle(sorted_requests)
            # æ‰“ä¹±åä½¿ç”¨ round-robin
            actual_strategy = 'round-robin'
        else:
            # æŒ‰ timestamp æ’åº
            sorted_requests = sorted(requests, key=lambda r: r.timestamp)
            actual_strategy = strategy
        
        base_timestamp = sorted_requests[0].timestamp if sorted_requests else 0
        
        semaphore = asyncio.Semaphore(max_concurrency)
        
        # é¢„å…ˆåˆ†é…æ¯ä¸ªè¯·æ±‚çš„ç›®æ ‡ URLï¼ˆé¿å…å¹¶å‘æ—¶çš„ç«äº‰æ¡ä»¶ï¼‰
        request_urls = []
        for i, req in enumerate(sorted_requests):
            if actual_strategy == 'hash':
                url = self.get_url_by_hash(req.hash_ids)
            elif actual_strategy == 'round-robin':
                url = self.base_urls[i % len(self.base_urls)]
            elif actual_strategy == 'random':
                url = random.choice(self.base_urls)
            else:
                url = self.base_urls[0]
            request_urls.append(url)
        
        # ç»Ÿè®¡åˆ†å‘æƒ…å†µ
        url_counts = {}
        for url in request_urls:
            url_counts[url] = url_counts.get(url, 0) + 1
        print(f"\nğŸ“Š è¯·æ±‚åˆ†å‘ç»Ÿè®¡:")
        for url, count in sorted(url_counts.items()):
            print(f"  {url}: {count} è¯·æ±‚ ({count/len(request_urls)*100:.1f}%)")
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            benchmark_start_time = time.time()
            
            for i, req in enumerate(sorted_requests):
                if use_timestamp:
                    target_delay = (req.timestamp - base_timestamp) / 1000.0 * time_scale
                    elapsed = time.time() - benchmark_start_time
                    wait_time = target_delay - elapsed
                    
                    if wait_time > 0:
                        await asyncio.sleep(wait_time)
                
                task = asyncio.create_task(
                    self.send_request(session, req, request_urls[i], semaphore)
                )
                tasks.append(task)
            
            print(f"\næ‰€æœ‰è¯·æ±‚å·²å‘é€ï¼Œç­‰å¾…å®Œæˆ...")
            results = await asyncio.gather(*tasks)
        
        total_time = time.time() - benchmark_start_time
        self._print_statistics(results, total_time)
        
        return results
    
    def _print_statistics(self, results: List[Dict], total_time: float):
        """æ‰“å°ç»Ÿè®¡ç»“æœ"""
        successful = [r for r in results if r.get('success')]
        failed = [r for r in results if not r.get('success')]
        
        print("\n" + "="*80)
        print(" Benchmark ç»“æœ")
        print("="*80)
        
        print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {len(results)}")
        print(f"  æˆåŠŸè¯·æ±‚: {len(successful)}")
        print(f"  å¤±è´¥è¯·æ±‚: {len(failed)}")
        print(f"  æˆåŠŸç‡: {len(successful)/len(results)*100:.1f}%")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"  å®é™… QPS: {len(results)/total_time:.2f}")
        
        if not successful:
            print("\nâš ï¸ æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚!")
            if failed:
                print("é”™è¯¯ç¤ºä¾‹:")
                for f in failed[:5]:
                    print(f"  - Req {f['request_id']}: {f.get('error', 'Unknown error')}")
            return
        
        # Token ç»Ÿè®¡
        total_prompt_tokens = sum(r['actual_prompt_tokens'] for r in successful)
        total_completion_tokens = sum(r['actual_completion_tokens'] for r in successful)
        
        print(f"\nğŸ“ Token ç»Ÿè®¡:")
        print(f"  æ€» Prompt Tokens: {total_prompt_tokens:,}")
        print(f"  æ€» Completion Tokens: {total_completion_tokens:,}")
        print(f"  å¹³å‡ Prompt é•¿åº¦: {total_prompt_tokens/len(successful):.1f}")
        print(f"  å¹³å‡ Completion é•¿åº¦: {total_completion_tokens/len(successful):.1f}")
        
        # ååé‡
        total_ttft = sum(r['ttft'] for r in successful if r['ttft'] > 0)
        total_decode_time = sum(r['decode_time'] for r in successful if r['decode_time'] > 0)
        
        print(f"\nğŸš€ ååé‡:")
        print(f"  ç«¯åˆ°ç«¯åå: {(total_prompt_tokens + total_completion_tokens)/total_time:.2f} tokens/s")
        if total_ttft > 0:
            print(f"  Prefill åå: {total_prompt_tokens/total_ttft:.2f} tokens/s")
        if total_decode_time > 0:
            print(f"  Decode åå: {total_completion_tokens/total_decode_time:.2f} tokens/s")
        
        # TTFT ç»Ÿè®¡
        ttfts = sorted([r['ttft'] for r in successful if r['ttft'] > 0])
        if ttfts:
            print(f"\nâ±ï¸ TTFT (Time to First Token):")
            print(f"  å¹³å‡: {sum(ttfts)/len(ttfts):.3f}s")
            print(f"  ä¸­ä½æ•°: {ttfts[len(ttfts)//2]:.3f}s")
            print(f"  P95: {ttfts[int(len(ttfts)*0.95)]:.3f}s")
            print(f"  P99: {ttfts[int(len(ttfts)*0.99)]:.3f}s")
            print(f"  æœ€å°: {ttfts[0]:.3f}s")
            print(f"  æœ€å¤§: {ttfts[-1]:.3f}s")
        
        # Decode é€Ÿåº¦ç»Ÿè®¡
        decode_speeds = sorted([r['decode_speed'] for r in successful if r['decode_speed'] > 0])
        if decode_speeds:
            print(f"\nâš¡ Decode é€Ÿåº¦:")
            print(f"  å¹³å‡: {sum(decode_speeds)/len(decode_speeds):.2f} tokens/s")
            print(f"  ä¸­ä½æ•°: {decode_speeds[len(decode_speeds)//2]:.2f} tokens/s")
        
        # æŒ‰èŠ‚ç‚¹ç»Ÿè®¡
        print(f"\nğŸ–¥ï¸ æ¯ä¸ªèŠ‚ç‚¹çš„è¯·æ±‚åˆ†å¸ƒ:")
        for base_url in self.base_urls:
            node_results = [r for r in successful if r['base_url'] == base_url]
            count = len(node_results)
            if count > 0:
                avg_ttft = sum(r['ttft'] for r in node_results) / count
                print(f"  {base_url}: {count} è¯·æ±‚ ({count/len(successful)*100:.1f}%), "
                      f"å¹³å‡TTFT: {avg_ttft:.3f}s")
        
        # Hash prefix åˆ†æ
        hash_lens = [r['hash_prefix_len'] for r in successful]
        if hash_lens:
            print(f"\nğŸ”— Hash Block åˆ†æ:")
            print(f"  å¹³å‡ block æ•°: {sum(hash_lens)/len(hash_lens):.1f}")
            print(f"  æœ€å° block æ•°: {min(hash_lens)}")
            print(f"  æœ€å¤§ block æ•°: {max(hash_lens)}")
            
            # ç»Ÿè®¡å”¯ä¸€ prefix
            unique_prefixes = set()
            for r in successful:
                prefix_tuple = tuple(r['hash_ids'])
                unique_prefixes.add(prefix_tuple)
            print(f"  å”¯ä¸€ prefix æ•°é‡: {len(unique_prefixes)}")
            
            # ç»Ÿè®¡ prefix å…±äº«æƒ…å†µ
            prefix_counts = defaultdict(int)
            for r in successful:
                # æ£€æŸ¥æ¯ä¸ªå¯èƒ½çš„å‰ç¼€é•¿åº¦
                for prefix_len in range(1, len(r['hash_ids']) + 1):
                    prefix = tuple(r['hash_ids'][:prefix_len])
                    prefix_counts[prefix] += 1
            
            # æ‰¾å‡ºå…±äº«æœ€å¤šçš„å‰ç¼€
            shared_prefixes = [(p, c) for p, c in prefix_counts.items() if c > 1]
            if shared_prefixes:
                shared_prefixes.sort(key=lambda x: -x[1])
                print(f"  å…±äº«çš„ prefix æ•°é‡: {len(shared_prefixes)}")
                print(f"  æœ€å¸¸å…±äº«çš„ prefix: {shared_prefixes[0][1]} æ¬¡ (é•¿åº¦ {len(shared_prefixes[0][0])})")
        
        # å¤±è´¥è¯·æ±‚åˆ†æ
        if failed:
            print(f"\nâŒ å¤±è´¥è¯·æ±‚åˆ†æ:")
            error_counts = defaultdict(int)
            for f in failed:
                error = f.get('error', 'Unknown')[:50]
                error_counts[error] += 1
            for error, count in sorted(error_counts.items(), key=lambda x: -x[1])[:5]:
                print(f"  {error}: {count} æ¬¡")
        
        print("\n" + "="*80)


def load_mooncake_trace(filepath: str, num_requests: int = None, start_line: int = 0) -> List[MooncakeRequest]:
    """åŠ è½½ Mooncake trace æ•°æ®"""
    requests = []
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i < start_line:
                continue
            if num_requests and len(requests) >= num_requests:
                break
            
            line = line.strip()
            if not line:
                continue
            
            try:
                data = json.loads(line)
                req = MooncakeRequest(
                    timestamp=data.get('timestamp', 0),
                    input_length=data.get('input_length', 100),
                    output_length=data.get('output_length', 50),
                    hash_ids=data.get('hash_ids', []),
                    request_id=len(requests),
                )
                requests.append(req)
            except json.JSONDecodeError as e:
                print(f"è­¦å‘Š: ç¬¬ {i+1} è¡Œ JSON è§£æå¤±è´¥: {e}")
                continue
    
    return requests


def main():
    parser = argparse.ArgumentParser(
        description='Mooncake Trace Benchmark for vLLM + FlexKV',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºç¡€ç”¨æ³•
  python benchmark_mooncake.py --trace /path/to/trace.jsonl \\
      --hosts 10.6.131.12:30001 \\
      --tokenizer /workspace/Qwen3-8B

  # å¤šèŠ‚ç‚¹ + hash ç­–ç•¥ï¼ˆæµ‹è¯• prefix cacheï¼‰
  python benchmark_mooncake.py --trace /path/to/trace.jsonl \\
      --hosts 10.6.131.12:30001,10.6.131.12:30002 \\
      --tokenizer /workspace/Qwen3-8B \\
      --strategy hash \\
      --block-size 512

  # é«˜å¹¶å‘å‹æµ‹
  python benchmark_mooncake.py --trace /path/to/trace.jsonl \\
      --hosts 10.6.131.12:30001 \\
      --tokenizer /workspace/Qwen3-8B \\
      --no-timestamp --concurrency 128
        """
    )
    
    parser.add_argument('--trace', type=str, required=True,
                        help='Mooncake trace JSONL æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--hosts', type=str, required=True,
                        help='èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ ¼å¼: host1:port1,host2:port2')
    parser.add_argument('--tokenizer', type=str, required=True,
                        help='Tokenizer è·¯å¾„ï¼ˆç”¨äºç”Ÿæˆ tokenï¼‰')
    parser.add_argument('--block-size', type=int, default=512,
                        help='æ¯ä¸ª hash block çš„ token æ•°é‡ï¼ˆé»˜è®¤: 512ï¼‰')
    parser.add_argument('--num-requests', type=int, default=None,
                        help='è¯·æ±‚æ•°é‡ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰')
    parser.add_argument('--start-line', type=int, default=0,
                        help='èµ·å§‹è¡Œå·')
    parser.add_argument('--concurrency', type=int, default=64,
                        help='æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--strategy', type=str, default='round-robin',
                        choices=['round-robin', 'random', 'hash', 'shuffle'],
                        help='è´Ÿè½½å‡è¡¡ç­–ç•¥')
    parser.add_argument('--model', type=str, default=None,
                        help='æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨ tokenizer è·¯å¾„ï¼‰')
    parser.add_argument('--time-scale', type=float, default=1.0,
                        help='æ—¶é—´ç¼©æ”¾å› å­')
    parser.add_argument('--no-timestamp', action='store_true',
                        help='ä¸æŒ‰æ—¶é—´æˆ³å‘é€')
    parser.add_argument('--use-token-ids', action='store_true',
                        help='ä½¿ç”¨ prompt_token_ids è€Œéæ–‡æœ¬ï¼ˆéœ€è¦ vLLM æ”¯æŒï¼‰')
    parser.add_argument('--timeout', type=int, default=300,
                        help='è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                        help='è¾“å‡ºç»“æœåˆ° JSON æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # è§£æ hosts
    hosts = []
    for host_str in args.hosts.split(','):
        host_str = host_str.strip()
        if ':' in host_str:
            host, port = host_str.split(':')
            hosts.append((host, int(port)))
        else:
            hosts.append((host_str, 8000))
    
    # åŠ è½½æ•°æ®
    print(f"åŠ è½½ Mooncake trace: {args.trace}")
    requests = load_mooncake_trace(args.trace, args.num_requests, args.start_line)
    print(f"åŠ è½½äº† {len(requests)} ä¸ªè¯·æ±‚")
    
    if not requests:
        print("é”™è¯¯: æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è¯·æ±‚!")
        return
    
    # æ‰“å°æ•°æ®é›†ç»Ÿè®¡
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    if requests[-1].timestamp > requests[0].timestamp:
        print(f"  æ—¶é—´èŒƒå›´: {requests[0].timestamp}ms - {requests[-1].timestamp}ms")
        print(f"  æŒç»­æ—¶é—´: {(requests[-1].timestamp - requests[0].timestamp)/1000:.1f}s")
    avg_input = sum(r.input_length for r in requests) / len(requests)
    avg_output = sum(r.output_length for r in requests) / len(requests)
    print(f"  å¹³å‡è¾“å…¥é•¿åº¦: {avg_input:.1f} tokens")
    print(f"  å¹³å‡è¾“å‡ºé•¿åº¦: {avg_output:.1f} tokens")
    avg_hash_len = sum(len(r.hash_ids) for r in requests) / len(requests)
    print(f"  å¹³å‡ hash block æ•°: {avg_hash_len:.1f}")
    print(f"  é¢„è®¡æ¯ä¸ª block: {args.block_size} tokens")
    print()
    
    # åˆ›å»º benchmark å®ä¾‹
    benchmark = MooncakeBenchmark(
        hosts=hosts,
        tokenizer_path=args.tokenizer,
        block_size=args.block_size,
        model_name=args.model,
        timeout=args.timeout,
        use_token_ids=args.use_token_ids,
    )
    
    # è¿è¡Œ benchmark
    use_timestamp = not args.no_timestamp and args.time_scale > 0
    results = asyncio.run(benchmark.run_benchmark(
        requests=requests,
        strategy=args.strategy,
        max_concurrency=args.concurrency,
        time_scale=args.time_scale,
        use_timestamp=use_timestamp,
    ))
    
    # ä¿å­˜ç»“æœ
    if args.output:
        # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å­—æ®µ
        serializable_results = []
        for r in results:
            sr = {k: v for k, v in r.items() if k != 'hash_ids' or isinstance(v, (list, tuple))}
            serializable_results.append(sr)
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == '__main__':
    main()
